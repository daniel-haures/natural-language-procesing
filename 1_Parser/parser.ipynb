{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libs.corpus import openConllu, check_projectivity\n",
    "import pyconll\n",
    "import pyconll.util\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "from torch.nn.utils.rnn import unpack_sequence\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pyconll.load_from_file('data/it_isdt-ud-train.conllu')\n",
    "train_prepocesed=[]\n",
    "for i,sent in enumerate(train):        \n",
    "    sentence_preprocesed=[]\n",
    "    for j,token in enumerate(sent):\n",
    "        if(token.head is not None):\n",
    "            sentence_preprocesed.append(token)\n",
    "    train_prepocesed.append(sentence_preprocesed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadind 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [100, 3], got [100, 66]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 330\u001b[0m\n\u001b[0;32m    328\u001b[0m oracle\u001b[38;5;241m=\u001b[39mOracle()\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m#parser.create_batches(100,train_prepocesed)\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 81\u001b[0m, in \u001b[0;36mOracle.train_on_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m predicted_output, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_packed_sequence(predicted_output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     79\u001b[0m batch_moves, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_packed_sequence(batch_moves, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 81\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_moves\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#backprop\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBack-prop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected target size [100, 3], got [100, 66]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0:SHIFT 1:RIGHTARC 2:LEFTARC\n",
    "moves = [0, 1, 2]\n",
    "\n",
    "\n",
    "#creiamo un oggetto Dependencies per salvare le dependencies\n",
    "class Dependencies(object):\n",
    "    def __init__(self,n):\n",
    "        self.n = n\n",
    "        self.heads = [None] * (n+1)\n",
    "        self.arcs = []\n",
    "    \n",
    "    def add_arc(self, head, child):\n",
    "        child=child\n",
    "        self.heads[child]=head\n",
    "        self.arcs.append((head,child))\n",
    "\n",
    "    def contains(self,head,child):\n",
    "        child=child\n",
    "        if self.heads[child]==head:\n",
    "            return True\n",
    "        else: return False\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class Oracle(object):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        #BERT encoder\n",
    "        encoder_name = \"dbmdz/bert-base-italian-xxl-cased\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "\n",
    "        #LSTM oracle\n",
    "        input_size = 3840  \n",
    "        hidden_size = 64\n",
    "        num_layers = 1\n",
    "        output_size = 3  \n",
    "        self.model = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=False,proj_size=output_size)\n",
    "        self.model.half()\n",
    "\n",
    "        #Oracle critenion and optimazation\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "    \n",
    "    def tokens(self,words,lemmas):\n",
    "        words_token=[102]\n",
    "        for i in range(0,len(lemmas)):\n",
    "            word_token=self.tokenizer.convert_tokens_to_ids(words[i])\n",
    "            lemma_token=self.tokenizer.convert_tokens_to_ids(lemmas[i])\n",
    "            if(word_token!=101):\n",
    "                words_token.append(word_token)\n",
    "            else:\n",
    "                words_token.append(lemma_token)\n",
    "        words_token.append(103)\n",
    "        return words_token\n",
    "    \n",
    "    def encode(self,words,lemmas):\n",
    "        words_token=self.tokens(words,lemmas)\n",
    "        #max_length = 64  # Example max length ????\n",
    "        padded_input_ids = words_token\n",
    "        input_tensor = torch.tensor([padded_input_ids])\n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(input_tensor)\n",
    "        return outputs.last_hidden_state\n",
    "    \n",
    "    #prende i batches dalla cartella, e allena il modello\n",
    "    def train_on_batches(self):\n",
    "        files = os.listdir(\"data/batches\")\n",
    "        for i in range(0,len(files)):\n",
    "            if(i==0):\n",
    "                print(\"Loadind \"+str(i))\n",
    "                batch_features,batch_moves=torch.load(f\"data/batches/{files[i]}\") \n",
    "\n",
    "                predicted_output, _ = self.model(batch_features)\n",
    "                predicted_output, _ = torch.nn.utils.rnn.pad_packed_sequence(predicted_output, batch_first=True)\n",
    "\n",
    "                batch_moves, _ = torch.nn.utils.rnn.pad_packed_sequence(batch_moves, batch_first=True)\n",
    "\n",
    "                loss = self.criterion(predicted_output, batch_moves)\n",
    "\n",
    "                #backprop\n",
    "                print(\"Back-prop\")\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                print(\"End back-prop\")\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    def score(features):\n",
    "        return None\n",
    "        \n",
    "\n",
    "class a():\n",
    "   def __init__(self, folder):\n",
    "       self.files = os.listdir(folder)\n",
    "       self.folder = folder\n",
    "   def __len__(self):\n",
    "       return len(self.files)\n",
    "   def __getitem__(self, idx):\n",
    "       return torch.load(f\"{self.folder}/{self.files[idx]}\")           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Parser(object):\n",
    "    model=Oracle()\n",
    "    \n",
    "    #applica la mossa andando ad aggiornare lo stack e l'indice del buffer  \n",
    "    def transition(self,move, stack, i, dependencies):\n",
    "        match move:\n",
    "            case 0:\n",
    "                stack.append(i)\n",
    "                return stack,i+1,dependencies\n",
    "            case 1:\n",
    "                dependencies.add_arc(stack[-2], stack.pop())\n",
    "                return stack,i,dependencies\n",
    "            case 2:\n",
    "                dependencies.add_arc(stack[-1], stack[-2])\n",
    "                stack.pop(-2)\n",
    "                return stack,i,dependencies\n",
    "            case _:\n",
    "                raise \"Wrong Move\"\n",
    "\n",
    "               #ritorna le mosse possibili che si possono applicare        \n",
    "    def get_valid_moves(i, n, stack_depth):\n",
    "        moves = []\n",
    "        if i <= n:\n",
    "            moves.append(0)\n",
    "        if stack_depth >= 2:\n",
    "            moves.append(1)\n",
    "            moves.append(2)\n",
    "        return moves\n",
    "\n",
    "    def parsing(self,words,tags):\n",
    "        if(len(words)!=len(tags)):\n",
    "            raise \"Wrong tag len\"\n",
    "        n=len(words)\n",
    "        deps=Dependencies(n)\n",
    "        stack=[0]\n",
    "        i_buffer=1\n",
    "        moves=self.get_valid_moves(i_buffer,n,len(stack))\n",
    "        while moves:\n",
    "            features = ['FeaturesDaScegliere']\n",
    "            scores = self.model.score(features)\n",
    "            next_move = max(moves, key=lambda move: scores[move])\n",
    "            stack,i_buffer,deps = self.transition(next_move, stack, i_buffer, deps)\n",
    "            moves = self.get_valid_moves(i,n,len(stack))\n",
    "        return deps\n",
    "    \n",
    "    #sceglie la mossa migliore da eseguire nel simulate_parse\n",
    "    def check_best(self,heads,stack,buffer,deps,i):\n",
    "        move=-1\n",
    "        if(len(stack)>=2):\n",
    "            children_list=[]\n",
    "            for child,head in enumerate(heads):\n",
    "                if head == stack[-1]:\n",
    "                    children_list.append(child)\n",
    "            if(((heads[stack[-1]])==(stack[-2])) and all([deps.contains(stack[-1],child) for child in children_list])):\n",
    "                move=1\n",
    "            if(heads[stack[-2]]==stack[-1]):\n",
    "                move=2\n",
    "        if(i<=len(buffer) and move==-1):\n",
    "            move=0\n",
    "        elif(i>len(buffer) and move==-1):\n",
    "            move=None\n",
    "        return move\n",
    "    \n",
    "    #fa reverse engineering, dato lo stato finale ricostruisce lo stack, buffer e le mosse\n",
    "    def simulate_parse(self,heads,buffer):\n",
    "        deps=Dependencies(len(buffer))\n",
    "        stack=[0]\n",
    "        moves=[]\n",
    "        buffers=[]\n",
    "        stacks=[]\n",
    "        i=1\n",
    "        best_move=self.check_best(heads,stack,buffer,deps,i)\n",
    "        while best_move!=None:\n",
    "            buffers.append(i)\n",
    "            stacks.append(stack[:])\n",
    "            moves.append(best_move)\n",
    "            stack,i,deps=self.transition(best_move,stack,i,deps)\n",
    "            best_move=self.check_best(heads,stack,buffer,deps,i)\n",
    "        if(i>len(buffer)):\n",
    "            return stacks,buffers,moves\n",
    "        else: return None \n",
    "\n",
    "    #prende gli ultimi tre elementi dello stack\n",
    "    def get_stack_context(self,list):\n",
    "        depth=len(list)\n",
    "\n",
    "        if depth >= 3:\n",
    "            return [list[-1], list[-2], list[-3]]\n",
    "        \n",
    "        elif depth >= 2:\n",
    "\n",
    "            return [list[-1], list[-2], -1]\n",
    "        \n",
    "        elif depth == 1:\n",
    "            return [list[-1], -1 , -1]\n",
    "        else:\n",
    "            return [-1, -1, -1]\n",
    "\n",
    "    def get_buffer_context(self,index,len_phrase):\n",
    "    #prende gli ultimi due elementi dell buffer\n",
    "        if(index==len_phrase):\n",
    "            return [index,-1]\n",
    "        elif(index>len_phrase):\n",
    "            return [-1,-1]\n",
    "        else: return [index,index+1]  \n",
    "\n",
    "    def flatten_embedded_features(self,matrix):\n",
    "        flat_list = torch.tensor([])\n",
    "        for row in matrix:\n",
    "            flat_list=torch.cat((flat_list,row))\n",
    "        return flat_list\n",
    "\n",
    "    def encode_moves(self,heads,phrase,phrases_lemma):\n",
    "        embeddings = self.model.encode(phrase,phrases_lemma)\n",
    "        stacks,buffers,moves=self.simulate_parse(heads,phrase)\n",
    "        root_embeding=torch.tensor(np.ones(768))\n",
    "        empty_embedding=torch.tensor(np.zeros(768))\n",
    "        embedded_features=[]\n",
    "        for i in range(0,len(stacks)):\n",
    "            stack_feature=self.get_stack_context(stacks[i])\n",
    "            for j,el in enumerate(stack_feature):\n",
    "                if(el>=1):\n",
    "                    stack_feature[j]=embeddings[0][el]\n",
    "                elif(el==0):\n",
    "                    stack_feature[j]=root_embeding\n",
    "                else:\n",
    "                    stack_feature[j]=empty_embedding\n",
    "            buffer_feature=self.get_buffer_context(buffers[i],len(phrase))\n",
    "            for j,el in enumerate(buffer_feature):\n",
    "                if(el>=1):\n",
    "                    buffer_feature[j]=embeddings[0][el]\n",
    "                else:\n",
    "                    buffer_feature[j]=empty_embedding\n",
    "\n",
    "            embedded_features.append(self.flatten_embedded_features(stack_feature+buffer_feature).to(torch.float16))#.to(torch.float16)\n",
    "        return embedded_features,moves\n",
    "    \n",
    "    def create_batches(self,batch_size,dataset):\n",
    "\n",
    "    #prende il dataset e per ogni frase genera il batch corrispettivo, genererà n=batch-size file    \n",
    "        batch_feature=[]\n",
    "        batch_moves=[]\n",
    "\n",
    "        index_batch=0\n",
    "        for i,sent in enumerate(dataset):     \n",
    "            if(i<1000):\n",
    "                heads=[-1]\n",
    "                words=[]\n",
    "                lemmas=[]\n",
    "\n",
    "                wrong_sent=0\n",
    "                for token in sent:\n",
    "                    if(token.head is None): wrong_sent=1\n",
    "                    if(token.form is None): wrong_sent=1\n",
    "                    if(token.lemma is None): wrong_sent=1\n",
    "\n",
    "                    heads.append(token.head)\n",
    "                    words.append(token.form)\n",
    "                    lemmas.append(token.lemma)\n",
    "                \n",
    "                if(wrong_sent==0):\n",
    "                    sent_features,sent_moves = self.encode_moves(heads,words,lemmas)\n",
    "                    sent_features=torch.stack(sent_features,dim=0)\n",
    "                    sent_moves=torch.tensor(sent_moves)\n",
    "                    batch_feature.append(sent_features)\n",
    "                    batch_moves.append(sent_moves)\n",
    "                \n",
    "\n",
    "                if(len(batch_moves)==batch_size): \n",
    "                    print(index_batch)\n",
    "                    packed_features=pack_sequence(batch_feature,enforce_sorted=False)\n",
    "                    packed_moves=pack_sequence(batch_moves,enforce_sorted=False)\n",
    "\n",
    "                    torch.save((packed_features,packed_moves), f\"data/batches/tensor{index_batch}.pt\")\n",
    "                    index_batch+=1\n",
    "                    batch_feature=[]\n",
    "                    batch_moves=[]\n",
    "\n",
    "\n",
    "#DA FARE:\n",
    "#1. Salvare batch da 50-100 frasi su file dati, ossia per ogni frase gli stati con relativi stack,buffer e move. \n",
    "#2. Importare batch per batch come nell'esempio di chat gpt.\n",
    "#3. Per ogni batch fare forward e back-prop di adam optimizer come nell'esempio di chat_gpt.\n",
    "\n",
    "\n",
    "parser = Parser()  \n",
    "phrase = \"book me the morning flight\".split()\n",
    "phrase2 = \"Book the flight Houston\".split()\n",
    "heads=[-1,0,1,5,5,1]\n",
    "heads2=[-1,0,3,1,3]\n",
    "stack=[0]\n",
    "parser= Parser()\n",
    "sstack,buffer,moves= parser.simulate_parse(heads,phrase)\n",
    "sstack2,buffer2,moves2= parser.simulate_parse(heads2,phrase2)\n",
    "\n",
    "\n",
    "#p1,m1=parser.encode_moves(heads,phrase,phrase)\n",
    "#p2,m2=parser.encode_moves(heads2,phrase2,phrase2)\n",
    "#print(m2)\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#p1 = torch.stack(p1, dim=0)\n",
    "#p2 = torch.stack(p2, dim=0)\n",
    "#print(p1)\n",
    "#print(p2)\n",
    "#input=pack_sequence([p1, p2])\n",
    "\n",
    "#input_size = 3840  # Each element in the sequence is a vector of size 2\n",
    "#hidden_size = 64\n",
    "#num_layers = 1\n",
    "#output_size = 3  # Example output size\n",
    "#model = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=False,proj_size=output_size)\n",
    "#model.half()\n",
    "#\n",
    "#output = model(input)\n",
    "#print(output)\n",
    "#print(\"OUTPUT\")\n",
    "#print(unpack_sequence(output[0]))\n",
    "\n",
    "oracle=Oracle()\n",
    "#parser.create_batches(100,train_prepocesed)\n",
    "oracle.train_on_batches()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
