{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","#/content/drive/MyDrive/batches"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_wq9I1GIEqz","executionInfo":{"status":"ok","timestamp":1715777799539,"user_tz":-120,"elapsed":31789,"user":{"displayName":"Lorenzo Giacometti","userId":"13895654272715125957"}},"outputId":"16d38089-5d20-4ae2-fe16-c5b3e343e5bd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hQLM7wlvH_GT","executionInfo":{"status":"ok","timestamp":1715777824626,"user_tz":-120,"elapsed":13184,"user":{"displayName":"Lorenzo Giacometti","userId":"13895654272715125957"}},"outputId":"0ff10058-3ccf-4d78-ef83-d3c34b73eba1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyconll\n","  Downloading pyconll-3.2.0-py3-none-any.whl (27 kB)\n","Installing collected packages: pyconll\n","Successfully installed pyconll-3.2.0\n","True\n","cuda\n"]}],"source":["#from libs.corpus import openConllu, check_projectivity\n","!pip install pyconll\n","import pyconll\n","import pyconll.util\n","import torch\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","from torch.nn.utils.rnn import pack_sequence, unpack_sequence\n","import os\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print( torch.cuda.is_available())\n","print(device)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"7jUj-grvH_GY","executionInfo":{"status":"ok","timestamp":1715777850033,"user_tz":-120,"elapsed":13195,"user":{"displayName":"Lorenzo Giacometti","userId":"13895654272715125957"}}},"outputs":[],"source":["\n","train = pyconll.load_from_file('/content/drive/MyDrive/magistrale/TLN/data/it_isdt-ud-train.conllu')\n","train_prepocesed=[]\n","for i,sent in enumerate(train):\n","    sentence_preprocesed=[]\n","    for j,token in enumerate(sent):\n","        if(token.head is not None):\n","            sentence_preprocesed.append(token)\n","    train_prepocesed.append(sentence_preprocesed)\n","\n"]},{"cell_type":"code","execution_count":87,"metadata":{"id":"TO4-DLHkH_Gd","executionInfo":{"status":"ok","timestamp":1715787259575,"user_tz":-120,"elapsed":470,"user":{"displayName":"Lorenzo Giacometti","userId":"13895654272715125957"}}},"outputs":[],"source":["\n","\n","\n","\n","\n","#0:SHIFT 1:RIGHTARC 2:LEFTARC\n","moves = [0, 1, 2]\n","\n","\n","#creiamo un oggetto Dependencies per salvare le dependencies\n","class Dependencies(object):\n","    def __init__(self,n):\n","        self.n = n\n","        self.heads = [None] * (n+1)\n","        self.arcs = []\n","\n","    def get_heads(self):\n","        return self.heads\n","\n","    def add_arc(self, head, child):\n","        child=child\n","        self.heads[child]=head\n","        self.arcs.append((head,child))\n","\n","    def contains(self,head,child):\n","        child=child\n","        if self.heads[child]==head:\n","            return True\n","        else: return False\n","\n","\n","\n","\n","\n","class Oracle(object):\n","\n","    def __init__(self) -> None:\n","        #BERT encoder\n","        encoder_name = \"dbmdz/bert-base-italian-xxl-cased\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n","        self.encoder = AutoModel.from_pretrained(encoder_name)\n","\n","        #LSTM oracle\n","        input_size = 4608\n","        hidden_size = 256\n","        num_layers = 1\n","        output_size = 3\n","        self.epoch = 12\n","        self.model = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=False,proj_size=output_size).to(device)\n","\n","        #Oracle critenion and optimazation\n","        self.criterion = torch.nn.CrossEntropyLoss()\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0005)\n","\n","    def tokens(self,words,lemmas):\n","        words_token=[102]\n","        for i in range(0,len(lemmas)):\n","            word_token=self.tokenizer.convert_tokens_to_ids(words[i])\n","            lemma_token=self.tokenizer.convert_tokens_to_ids(lemmas[i])\n","            if(word_token!=101):\n","                words_token.append(word_token)\n","            else:\n","                words_token.append(lemma_token)\n","        words_token.append(103)\n","        return words_token\n","\n","    def encode(self,words,lemmas):\n","        words_token=self.tokens(words,lemmas)\n","        #max_length = 64  # Example max length ????\n","        padded_input_ids = words_token\n","        input_tensor = torch.tensor([padded_input_ids])\n","        with torch.no_grad():\n","            outputs = self.encoder(input_tensor)\n","        return outputs.last_hidden_state\n","\n","    #prende i batches dalla cartella, e allena il modello\n","    def train_on_batches(self):\n","        files = os.listdir(\"/content/drive/MyDrive/magistrale/TLN/batches\")\n","        for k in range(0,self.epoch):\n","          for i in range(0,len(files)):\n","              if(i<30):\n","\n","                  print(\"Loadind \"+str(i))\n","                  batch_features,batch_moves=torch.load(f\"/content/drive/MyDrive/magistrale/TLN/batches/tensor{i}.pt\")\n","                  batch_features=unpack_sequence(batch_features)\n","                  batch_moves=unpack_sequence(batch_moves)\n","                  for j in range(0,len(batch_moves)):\n","                      predicted_output, _ = self.model(batch_features[j].to(device))\n","                      loss = self.criterion(predicted_output, batch_moves[j].to(device))\n","                      self.optimizer.zero_grad()\n","                      loss.backward()\n","                      self.optimizer.step()\n","\n","\n","    def score(self,features):\n","        predicted_output,_ = self.model(features.to(torch.float32).to(device))\n","        return predicted_output\n","\n","    #prende gli ultimi tre elementi dello stack\n","    def get_stack_context(self,list):\n","        depth=len(list)\n","\n","        if depth >= 3:\n","            return [list[-1], list[-2], list[-3]]\n","\n","        elif depth >= 2:\n","\n","            return [list[-1], list[-2], -1]\n","\n","        elif depth == 1:\n","            return [list[-1], -1 , -1]\n","        else:\n","            return [-1, -1, -1]\n","\n","    #prende gli ultimi due elementi dell buffer\n","    def get_buffer_context(self,index,len_phrase):\n","        if(index==len_phrase-1):\n","            return [index,index+1,-1]\n","        elif(index==len_phrase):\n","            return [index,-1,-1]\n","        elif(index>len_phrase):\n","            return [-1,-1,-1]\n","        else: return [index,index+1,index+2]\n","\n","    def flatten_embedded_features(self,matrix):\n","        flat_list = torch.tensor([])\n","        for row in matrix:\n","            flat_list=torch.cat((flat_list,row))\n","        return flat_list\n","\n","    def extract_features(self,phrase,phrases_lemma,stacks,buffers):\n","        embeddings = self.encode(phrase,phrases_lemma)\n","        root_embeding=embeddings[0][0]\n","        empty_embedding=torch.tensor(np.zeros(768))\n","        embedded_features=[]\n","        for i in range(0,len(stacks)):\n","            stack_feature=self.get_stack_context(stacks[i])\n","            for j,el in enumerate(stack_feature):\n","                if(el>=1):\n","                    stack_feature[j]=embeddings[0][el]\n","                elif(el==0):\n","                    stack_feature[j]=root_embeding\n","                else:\n","                    stack_feature[j]=empty_embedding\n","            buffer_feature=self.get_buffer_context(buffers[i],len(phrase))\n","            for j,el in enumerate(buffer_feature):\n","                if(el>=1):\n","                    buffer_feature[j]=embeddings[0][el]\n","                else:\n","                    buffer_feature[j]=empty_embedding\n","            embedded_features.append(self.flatten_embedded_features(stack_feature+buffer_feature))\n","        return embedded_features\n","\n","\n","\n","class Parser(object):\n","    def __init__(self,oracle):\n","        self.oracle=oracle\n","\n","    #applica la mossa andando ad aggiornare lo stack e l'indice del buffer\n","    def transition(self,move, stack, i, dependencies):\n","        match move:\n","            case 0:\n","                stack.append(i)\n","                return stack,i+1,dependencies\n","            case 1:\n","                dependencies.add_arc(stack[-2], stack.pop())\n","                return stack,i,dependencies\n","            case 2:\n","                dependencies.add_arc(stack[-1], stack[-2])\n","                stack.pop(-2)\n","                return stack,i,dependencies\n","            case _:\n","                raise \"Wrong Move\"\n","\n","    #ritorna le mosse possibili che si possono applicare\n","    def get_valid_moves(self,i, n, stack_depth):\n","        moves = []\n","        if i <= n:\n","            moves.append(0)\n","        if stack_depth >= 2:\n","            moves.append(1)\n","            moves.append(2)\n","        return moves\n","\n","    def parsing(self,words,phrases_lemma):\n","        n=len(words)\n","        deps=Dependencies(n)\n","        stack=[0]\n","        i_buffer=1\n","        moves=self.get_valid_moves(i_buffer,n,len(stack))\n","        old_stack=[]\n","        old_buffer=[]\n","        memory=1\n","        while moves:\n","            features = self.oracle.extract_features(words,phrases_lemma,[stack],[i_buffer])\n","            features=torch.stack(features)\n","            scores = self.oracle.score(features)\n","            scores=scores[-1].tolist()\n","            next_move = max(moves, key=lambda move: scores[move])\n","            stack,i_buffer,deps = self.transition(next_move, stack, i_buffer, deps)\n","            moves = self.get_valid_moves(i_buffer,n,len(stack))\n","\n","            if(len(old_stack)<memory):\n","                old_stack.append(stack)\n","                old_buffer.append(i_buffer)\n","            else:\n","                old_stack.pop(0)\n","                old_buffer.pop(0)\n","                old_stack.append(stack)\n","                old_buffer.append(i_buffer)\n","\n","        return deps\n","\n","    #sceglie la mossa migliore da eseguire nel simulate_parse\n","    def check_best(self,heads,stack,buffer,deps,i):\n","        move=-1\n","        if(len(stack)>=2):\n","            children_list=[]\n","            for child,head in enumerate(heads):\n","                if head == stack[-1]:\n","                    children_list.append(child)\n","            if(heads[stack[-2]]==stack[-1]):\n","                move=2\n","            if(((heads[stack[-1]])==(stack[-2])) and all([deps.contains(stack[-1],child) for child in children_list])):\n","                move=1\n","        if(i<=len(buffer) and move==-1):\n","            move=0\n","        elif(i>len(buffer) and move==-1):\n","            move=None\n","        return move\n","    #fa reverse engineering, dato lo stato finale ricostruisce lo stack, buffer e le mosse\n","    def simulate_parse(self,heads,buffer):\n","        deps=Dependencies(len(buffer))\n","        stack=[0]\n","        moves=[]\n","        buffers=[]\n","        stacks=[]\n","        i=1\n","        best_move=self.check_best(heads,stack,buffer,deps,i)\n","        while best_move!=None:\n","            buffers.append(i)\n","            stacks.append(stack[:])\n","            moves.append(best_move)\n","            stack,i,deps=self.transition(best_move,stack,i,deps)\n","            best_move=self.check_best(heads,stack,buffer,deps,i)\n","        if(i>len(buffer)):\n","            return stacks,buffers,moves\n","        else: return None\n","\n","\n"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9EEaWrtbH_Gh","outputId":"72748263-c1b0-48ff-bc24-b314a721f4ff","executionInfo":{"status":"ok","timestamp":1715787618141,"user_tz":-120,"elapsed":355563,"user":{"displayName":"Lorenzo Giacometti","userId":"13895654272715125957"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n","Loadind 17\n","Loadind 18\n","Loadind 19\n","Loadind 20\n","Loadind 21\n","Loadind 22\n","Loadind 23\n","Loadind 24\n","Loadind 25\n","Loadind 26\n","Loadind 27\n","Loadind 28\n","Loadind 29\n"]}],"source":["def encode_moves(oracle,parser,heads,phrase,phrases_lemma):\n","    stacks,buffers,moves=parser.simulate_parse(heads,phrase)\n","    embedded_features=oracle.extract_features(phrase,phrases_lemma,stacks,buffers)\n","\n","\n","    expanded_moves=[]\n","    for move in moves:\n","        if(move==0): expanded_moves.append(torch.tensor([1,0,0]))\n","        if(move==1): expanded_moves.append(torch.tensor([0,1,0]))\n","        if(move==2): expanded_moves.append(torch.tensor([0,0,1]))\n","\n","    return embedded_features,expanded_moves\n","\n","#prende il dataset e per ogni frase genera il batch corrispettivo, genererà n=batch-size file\n","def create_batches(oracle,parser,batch_size,dataset):\n","    batch_feature=[]\n","    batch_moves=[]\n","\n","    index_batch=0\n","    for sent in dataset:\n","        if(index_batch<30):\n","            heads=[-1]\n","            words=[]\n","            lemmas=[]\n","\n","            wrong_sent=0\n","            for token in sent:\n","                if(token.head is None): wrong_sent=1\n","                if(token.form is None): wrong_sent=1\n","                if(token.lemma is None): wrong_sent=1\n","\n","                heads.append(int(token.head))\n","                words.append(token.form)\n","                lemmas.append(token.lemma)\n","\n","            if(wrong_sent==0):\n","                sent_features,sent_moves = encode_moves(oracle,parser,heads,words,lemmas)\n","                sent_features=torch.stack(sent_features,dim=0).to(torch.float32)\n","                sent_moves=torch.stack(sent_moves).to(torch.float32)\n","                batch_feature.append(sent_features)\n","                batch_moves.append(sent_moves)\n","\n","\n","            if(len(batch_moves)==batch_size):\n","                #print(sent_moves)\n","                print(index_batch)\n","                packed_features=pack_sequence(batch_feature,enforce_sorted=False)\n","                packed_moves=pack_sequence(batch_moves,enforce_sorted=False)\n","\n","\n","                torch.save((packed_features,packed_moves), f\"/content/drive/MyDrive/magistrale/TLN/batches/tensor{index_batch}.pt\")\n","                index_batch+=1\n","                batch_feature=[]\n","                batch_moves=[]\n","#DA FARE:\n","#1. Salvare batch da 50-100 frasi su file dati, ossia per ogni frase gli stati con relativi stack,buffer e move.\n","#2. Importare batch per batch come nell'esempio di chat gpt.\n","#3. Per ogni batch fare forward e back-prop di adam optimizer come nell'esempio di chat_gpt.\n","\n","\n","\n","#p1,m1=parser.encode_moves(heads,phrase,phrase)\n","#p2,m2=parser.encode_moves(heads2,phrase2,phrase2)\n","#print(m2)\n","#torch.set_printoptions(profile=\"full\")\n","#p1 = torch.stack(p1, dim=0)\n","#p2 = torch.stack(p2, dim=0)\n","#print(p1)\n","#print(p2)\n","#input=pack_sequence([p1, p2])\n","\n","#input_size = 3840  # Each element in the sequence is a vector of size 2\n","#hidden_size = 64\n","#num_layers = 1\n","#output_size = 3  # Example output size\n","#model = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=False,proj_size=output_size)\n","#model.half()\n","#\n","#output = model(input)\n","#print(output)\n","#print(\"OUTPUT\")\n","#print(unpack_sequence(output[0]))\n","\n","\n","oracle = Oracle()\n","oracle.train_on_batches()\n","#create_batches(oracle,parser,100,train_prepocesed)\n","\n"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zik-41O6H_Gj","executionInfo":{"status":"ok","timestamp":1715787620624,"user_tz":-120,"elapsed":2488,"user":{"displayName":"Lorenzo Giacometti","userId":"13895654272715125957"}},"outputId":"853f3dc4-2abb-49dd-cdd5-346930276038"},"outputs":[{"output_type":"stream","name":"stdout","text":["[None, 2, 3, 4, 0, 6, 4, 8, 4, 10, 8, 12, 10, 4]\n","[[0], [0, 1], [0, 1, 2], [0, 2], [0, 2, 3], [0, 2, 3, 4], [0, 2, 4], [0, 4], [0, 4, 5], [0, 4, 5, 6], [0, 4, 6], [0, 4], [0, 4, 7], [0, 4, 7, 8], [0, 4, 8], [0, 4, 8, 9], [0, 4, 8, 9, 10], [0, 4, 8, 10], [0, 4, 8, 10, 11], [0, 4, 8, 10, 11, 12], [0, 4, 8, 10, 12], [0, 4, 8, 10], [0, 4, 8], [0, 4], [0, 4, 13], [0, 4]]\n","[0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 0, 2, 1, 1, 1, 0, 1, 1]\n"]}],"source":["\n","\n","parser = Parser(oracle)\n","phrases1= \"Si chiamava Prototaxites .\".split()\n","phrases2= \"In la mia mente , almeno , è così .\".split()\n","phrases3=\" I turisti possono usare le barche per visitare alcune zone di Roma .\".split()\n","phrases4=\"Il più largo organismo a il mondo è un tappeto miceliale , di lo spessore di una parete cellulare .\".split()\n","heads1=[-1,2,0,2,2]\n","heads2=[-1,4,4,4,9,4,4,6,9,0,9]\n","heads3=[-1,2,4,4,0,6,4,8,4,10,8,12,10,4]\n","heads4=[-1,4,3,4,0,7,7,4,4,10,4,4,4,15,15,4,18,18,15,18,4]\n","deps = parser.parsing(phrases3,phrases3)\n","print(deps.get_heads())\n","stacks,buffers,moves=parser.simulate_parse(heads3,phrases3)\n","print(stacks)\n","print(moves)\n","\n","\n"]},{"cell_type":"code","source":["def accuracy_on_test():\n","  #Load e preprocessing\n","  test = pyconll.load_from_file('/content/drive/MyDrive/magistrale/TLN/data/it_isdt-ud-test.conllu')\n","  test_prepocesed=[]\n","  for i,sent in enumerate(test):\n","      sentence_preprocesed=[]\n","      for j,token in enumerate(sent):\n","          if(token.head is not None):\n","              sentence_preprocesed.append(token)\n","      test_prepocesed.append(sentence_preprocesed)\n","\n","  #Accuracy\n","  total_tokens=0\n","  total_wrong_tokens=0\n","  for i,sent in enumerate(test):\n","    if(i<100):\n","      print(i)\n","      heads=[-1]\n","      words=[]\n","      lemmas=[]\n","\n","      wrong_sent=0\n","      for token in sent:\n","        if(token.head is None): wrong_sent=1\n","        else:\n","          heads.append(int(token.head))\n","        if(token.form is None): wrong_sent=1\n","        if(token.lemma is None): wrong_sent=1\n","\n","\n","        words.append(token.form)\n","        lemmas.append(token.lemma)\n","\n","      if(wrong_sent!=1):\n","        predicted_deps=parser.parsing(words,lemmas)\n","        predicted_heads=predicted_deps.get_heads()\n","        sent_tokens=len(words)\n","        sent_wrong_tokens=0\n","        for j in range(1,len(predicted_heads)):\n","          if(predicted_heads[j] is not None):\n","            if(predicted_heads[j]!=heads[j]):\n","              sent_wrong_tokens+=1\n","          else:\n","            sent_wrong_tokens+=1\n","        total_tokens+=sent_tokens\n","        total_wrong_tokens+=sent_wrong_tokens\n","\n","  accuracy=1-(total_wrong_tokens/total_tokens)\n","  return accuracy\n","\n","print(accuracy_on_test())\n","\n","\n","\n","\n","\n"],"metadata":{"id":"SFErqggjluhf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715787742339,"user_tz":-120,"elapsed":121721,"user":{"displayName":"Lorenzo Giacometti","userId":"13895654272715125957"}},"outputId":"27483b82-5a75-408a-9598-4d978f4211a9"},"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","0.7773722627737226\n"]}]},{"cell_type":"markdown","source":["hidden 64, epoch 1, lr 0.001, memory 0, bidir False, batches 30 0.7\n","hidden 64, epoch 1, lr 0.01, memory 0, bidir False, batches 30 0.55 overfitting\n","hidden 64, epoch 1, lr 0.0001, memory 0, bidir False, batches 0.58 underfitting\n","\n","hidden 64, epoch 3, lr 0.001, memory 0, bidir False, batches 30 0.74\n","hidden 64, epoch 4, lr 0.001, memory 0, bidir False, batches 30 0.78 top\n","hidden 64, epoch 4, lr 0.001, memory 3, bidir False, batches 30 0.64 !\n","\n","hidden 64, epoch 4, lr 0.001, memory 0, bidir False, batches 60 0.75  overfitting\n","hidden 64, epoch 3, lr 0.001, memory 0, bidir False, batches 60 0.76  overfitting\n","hidden 64, epoch 2, lr 0.001, memory 0, bidir False, batches 60 0.78  uguale a prima\n","hidden 64, epoch 1, lr 0.001, memory 0, bidir False, batches 60 0.71  underfitting\n","\n","hidden 64, epoch 2, lr 0.0005, memory 0, bidir False, batches 120 0.78  \n","hidden 64, epoch 3, lr 0.0005, memory 0, bidir False, batches 120 0.78  \n","hidden 128, epoch 3, lr 0.0005, memory 0, bidir False, batches 120 0.815  top\n","hidden 512, epoch 4, lr 0.0005, memory 0, bidir False, batches 120 0.83  top"],"metadata":{"id":"p8xFzbYo_UAH"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}