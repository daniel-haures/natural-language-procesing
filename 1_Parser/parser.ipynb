{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from libs.corpus import openConllu, check_projectivity\n",
    "import pyconll\n",
    "import pyconll.util\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pack_sequence, unpack_sequence\n",
    "import os\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pyconll.load_from_file('data/it_isdt-ud-train.conllu')\n",
    "train_prepocesed=[]\n",
    "for i,sent in enumerate(train):        \n",
    "    sentence_preprocesed=[]\n",
    "    for j,token in enumerate(sent):\n",
    "        if(token.head is not None):\n",
    "            sentence_preprocesed.append(token)\n",
    "    train_prepocesed.append(sentence_preprocesed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0:SHIFT 1:RIGHTARC 2:LEFTARC\n",
    "moves = [0, 1, 2]\n",
    "\n",
    "\n",
    "#creiamo un oggetto Dependencies per salvare le dependencies\n",
    "class Dependencies(object):\n",
    "    def __init__(self,n):\n",
    "        self.n = n\n",
    "        self.heads = [None] * (n+1)\n",
    "        self.arcs = []\n",
    "    \n",
    "    def get_heads(self):\n",
    "        return self.heads\n",
    "    \n",
    "    def add_arc(self, head, child):\n",
    "        child=child\n",
    "        self.heads[child]=head\n",
    "        self.arcs.append((head,child))\n",
    "\n",
    "    def contains(self,head,child):\n",
    "        child=child\n",
    "        if self.heads[child]==head:\n",
    "            return True\n",
    "        else: return False\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class Oracle(object):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        #BERT encoder\n",
    "        encoder_name = \"dbmdz/bert-base-italian-xxl-cased\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "\n",
    "        #LSTM oracle\n",
    "        input_size = 4608  \n",
    "        hidden_size = 64\n",
    "        num_layers = 1\n",
    "        output_size = 3  \n",
    "        self.model = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=False,proj_size=output_size)\n",
    "\n",
    "        #Oracle critenion and optimazation\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "    \n",
    "    def tokens(self,words,lemmas):\n",
    "        words_token=[102]\n",
    "        for i in range(0,len(lemmas)):\n",
    "            word_token=self.tokenizer.convert_tokens_to_ids(words[i])\n",
    "            lemma_token=self.tokenizer.convert_tokens_to_ids(lemmas[i])\n",
    "            if(word_token!=101):\n",
    "                words_token.append(word_token)\n",
    "            else:\n",
    "                words_token.append(lemma_token)\n",
    "        words_token.append(103)\n",
    "        return words_token\n",
    "    \n",
    "    def encode(self,words,lemmas):\n",
    "        words_token=self.tokens(words,lemmas)\n",
    "        #max_length = 64  # Example max length ????\n",
    "        padded_input_ids = words_token\n",
    "        input_tensor = torch.tensor([padded_input_ids])\n",
    "        with torch.no_grad():\n",
    "            outputs = self.encoder(input_tensor)\n",
    "        return outputs.last_hidden_state\n",
    "    \n",
    "    #prende i batches dalla cartella, e allena il modello\n",
    "    def train_on_batches(self):\n",
    "        files = os.listdir(\"data/batches\")\n",
    "        for i in range(0,len(files)):\n",
    "            if(i==0):\n",
    "                \n",
    "                print(\"Loadind \"+str(i))\n",
    "                batch_features,batch_moves,batch_pos=torch.load(f\"data/batches/{files[i]}\") \n",
    "                batch_features=unpack_sequence(batch_features)\n",
    "                batch_moves=unpack_sequence(batch_moves)\n",
    "                for j in range(0,len(batch_moves)):\n",
    "                    predicted_output, _ = self.model(batch_features[j])\n",
    "                    loss = self.criterion(predicted_output, batch_moves[j])\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "    \n",
    "    def score(self,features):\n",
    "        predicted_output,_ = self.model(features)\n",
    "        return predicted_output\n",
    "    \n",
    "    #prende gli ultimi tre elementi dello stack\n",
    "    def get_stack_context(self,list):\n",
    "        depth=len(list)\n",
    "\n",
    "        if depth >= 3:\n",
    "            return [list[-1], list[-2], list[-3]]\n",
    "        \n",
    "        elif depth >= 2:\n",
    "\n",
    "            return [list[-1], list[-2], -1]\n",
    "        \n",
    "        elif depth == 1:\n",
    "            return [list[-1], -1 , -1]\n",
    "        else:\n",
    "            return [-1, -1, -1]\n",
    "\n",
    "    #prende gli ultimi due elementi dell buffer\n",
    "    def get_buffer_context(self,index,len_phrase):\n",
    "        if(index==len_phrase-1):\n",
    "            return [index,index+1,-1]\n",
    "        elif(index==len_phrase):\n",
    "            return [index,-1,-1]\n",
    "        elif(index>len_phrase):\n",
    "            return [-1,-1,-1]\n",
    "        else: return [index,index+1,index+2]  \n",
    "\n",
    "    def flatten_embedded_features(self,matrix):\n",
    "        flat_list = torch.tensor([])\n",
    "        for row in matrix:\n",
    "            flat_list=torch.cat((flat_list,row))\n",
    "        return flat_list\n",
    "\n",
    "    def extract_features(self,phrase,phrases_lemma,stacks,buffers):\n",
    "        embeddings = self.encode(phrase,phrases_lemma)\n",
    "        root_embeding=embeddings[0][0]\n",
    "        empty_embedding=torch.tensor(np.zeros(768))\n",
    "        embedded_features=[]\n",
    "        for i in range(0,len(stacks)):\n",
    "            stack_feature=self.get_stack_context(stacks[i])\n",
    "            for j,el in enumerate(stack_feature):\n",
    "                if(el>=1):\n",
    "                    stack_feature[j]=embeddings[0][el]\n",
    "                elif(el==0):\n",
    "                    stack_feature[j]=root_embeding\n",
    "                else:\n",
    "                    stack_feature[j]=empty_embedding\n",
    "            buffer_feature=self.get_buffer_context(buffers[i],len(phrase))\n",
    "            for j,el in enumerate(buffer_feature):\n",
    "                if(el>=1):\n",
    "                    buffer_feature[j]=embeddings[0][el]\n",
    "                else:\n",
    "                    buffer_feature[j]=empty_embedding\n",
    "            embedded_features.append(self.flatten_embedded_features(stack_feature+buffer_feature))\n",
    "        return embedded_features\n",
    "\n",
    "\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self,oracle):\n",
    "        self.oracle=oracle\n",
    "\n",
    "    #applica la mossa andando ad aggiornare lo stack e l'indice del buffer  \n",
    "    def transition(self,move, stack, i, dependencies):\n",
    "        match move:\n",
    "            case 0:\n",
    "                stack.append(i)\n",
    "                return stack,i+1,dependencies\n",
    "            case 1:\n",
    "                dependencies.add_arc(stack[-2], stack.pop())\n",
    "                return stack,i,dependencies\n",
    "            case 2:\n",
    "                dependencies.add_arc(stack[-1], stack[-2])\n",
    "                stack.pop(-2)\n",
    "                return stack,i,dependencies\n",
    "            case _:\n",
    "                raise \"Wrong Move\"\n",
    "\n",
    "    #ritorna le mosse possibili che si possono applicare        \n",
    "    def get_valid_moves(self,i, n, stack_depth):\n",
    "        moves = []\n",
    "        if i <= n:\n",
    "            moves.append(0)\n",
    "        if stack_depth >= 2:\n",
    "            moves.append(1)\n",
    "            moves.append(2)\n",
    "        return moves\n",
    "\n",
    "    def parsing(self,words,phrases_lemma):\n",
    "        n=len(words)\n",
    "        deps=Dependencies(n)\n",
    "        stack=[0]\n",
    "        i_buffer=1\n",
    "        moves=self.get_valid_moves(i_buffer,n,len(stack))\n",
    "        old_stack=[]\n",
    "        old_buffer=[]\n",
    "        memory=2\n",
    "        while moves:\n",
    "            features = self.oracle.extract_features(words,phrases_lemma,[stack],[i_buffer])\n",
    "            features=torch.stack(features)\n",
    "            scores = self.oracle.score(features)\n",
    "            print(scores)\n",
    "            scores=scores[-1].tolist()\n",
    "            print(scores)\n",
    "            next_move = max(moves, key=lambda move: scores[move])\n",
    "            stack,i_buffer,deps = self.transition(next_move, stack, i_buffer, deps)\n",
    "            moves = self.get_valid_moves(i_buffer,n,len(stack))\n",
    "\n",
    "            if(len(old_stack)<memory):\n",
    "                old_stack.append(stack)\n",
    "                old_buffer.append(i_buffer)\n",
    "            else:\n",
    "                old_stack.pop(0)\n",
    "                old_buffer.pop(0)\n",
    "                old_stack.append(stack)\n",
    "                old_buffer.append(i_buffer)\n",
    "\n",
    "        return deps\n",
    "    \n",
    "    #sceglie la mossa migliore da eseguire nel simulate_parse\n",
    "    def check_best(self,heads,stack,buffer,deps,i):\n",
    "        move=-1\n",
    "        if(len(stack)>=2):\n",
    "            children_list=[]\n",
    "            for child,head in enumerate(heads):\n",
    "                if head == stack[-1]:\n",
    "                    children_list.append(child)\n",
    "            if(heads[stack[-2]]==stack[-1]):\n",
    "                move=2\n",
    "            if(((heads[stack[-1]])==(stack[-2])) and all([deps.contains(stack[-1],child) for child in children_list])):\n",
    "                move=1\n",
    "        if(i<=len(buffer) and move==-1):\n",
    "            move=0\n",
    "        elif(i>len(buffer) and move==-1):\n",
    "            move=None\n",
    "        return move\n",
    "    \n",
    "    #fa reverse engineering, dato lo stato finale ricostruisce lo stack, buffer e le mosse\n",
    "    def simulate_parse(self,heads,buffer):\n",
    "        deps=Dependencies(len(buffer))\n",
    "        stack=[0]\n",
    "        moves=[]\n",
    "        buffers=[]\n",
    "        stacks=[]\n",
    "        i=1\n",
    "        best_move=self.check_best(heads,stack,buffer,deps,i)\n",
    "        while best_move!=None:\n",
    "            buffers.append(i)\n",
    "            stacks.append(stack[:])\n",
    "            moves.append(best_move)\n",
    "            stack,i,deps=self.transition(best_move,stack,i,deps)\n",
    "            best_move=self.check_best(heads,stack,buffer,deps,i)\n",
    "        if(i>len(buffer)):\n",
    "            return stacks,buffers,moves\n",
    "        else: return None \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "def encode_moves(oracle,parser,heads,phrase,phrases_lemma):\n",
    "    stacks,buffers,moves=parser.simulate_parse(heads,phrase)\n",
    "    embedded_features=oracle.extract_features(phrase,phrases_lemma,stacks,buffers)\n",
    "    \n",
    "\n",
    "    expanded_moves=[]\n",
    "    for move in moves:\n",
    "        if(move==0): expanded_moves.append(torch.tensor([1,0,0]))\n",
    "        if(move==1): expanded_moves.append(torch.tensor([0,1,0]))\n",
    "        if(move==2): expanded_moves.append(torch.tensor([0,0,1]))\n",
    "\n",
    "    return embedded_features,expanded_moves\n",
    "\n",
    "#prende il dataset e per ogni frase genera il batch corrispettivo, genererà n=batch-size file    \n",
    "def create_batches(oracle,parser,batch_size,dataset):\n",
    "    batch_feature=[]\n",
    "    batch_moves=[]\n",
    "    batch_pos=[]\n",
    "\n",
    "    pos_tags = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN','VERB','ADP','AUX','CCONJ','DET','NUM','PART','PRON','SCONJ','PUNCT','SYM','X']\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le = le.fit(pos_tags)\n",
    "\n",
    "    index_batch=0\n",
    "    for sent in dataset:     \n",
    "        if(index_batch<200):\n",
    "            heads=[-1]\n",
    "            words=[]\n",
    "            lemmas=[]\n",
    "            pos=[]\n",
    "\n",
    "\n",
    "            wrong_sent=0\n",
    "            for token in sent:\n",
    "                if(token.head is None): wrong_sent=1\n",
    "                if(token.form is None): wrong_sent=1\n",
    "                if(token.lemma is None): wrong_sent=1\n",
    "                if(token.upos is None): \n",
    "                    wrong_sent=1\n",
    "                    print(\"wrongpos\")\n",
    "\n",
    "                heads.append(int(token.head))\n",
    "                words.append(token.form)\n",
    "                lemmas.append(token.lemma)\n",
    "                pos.append(token.upos)\n",
    "            \n",
    "            if(wrong_sent==0):\n",
    "                sent_features,sent_moves = encode_moves(oracle,parser,heads,words,lemmas)\n",
    "                sent_features=torch.stack(sent_features,dim=0).to(torch.float32)\n",
    "                sent_moves=torch.stack(sent_moves).to(torch.float32)\n",
    "                batch_feature.append(sent_features)\n",
    "                batch_moves.append(sent_moves)\n",
    "\n",
    "                pos_int=le.transform(pos)\n",
    "                batch_pos.append(torch.tensor(pos_int))\n",
    "                \n",
    "            \n",
    "\n",
    "            if(len(batch_moves)==batch_size): \n",
    "                #print(sent_moves)\n",
    "                print(index_batch)\n",
    "                packed_features=pack_sequence(batch_feature,enforce_sorted=False)\n",
    "                packed_moves=pack_sequence(batch_moves,enforce_sorted=False)\n",
    "                packed_pos=pack_sequence(batch_pos,enforce_sorted=False)\n",
    "\n",
    "\n",
    "                torch.save((packed_features,packed_moves,packed_pos), f\"data/batches/tensor{index_batch}.pt\")\n",
    "                index_batch+=1\n",
    "                batch_feature=[]\n",
    "                batch_moves=[]\n",
    "#DA FARE:\n",
    "#1. Salvare batch da 50-100 frasi su file dati, ossia per ogni frase gli stati con relativi stack,buffer e move. \n",
    "#2. Importare batch per batch come nell'esempio di chat gpt.\n",
    "#3. Per ogni batch fare forward e back-prop di adam optimizer come nell'esempio di chat_gpt.\n",
    "\n",
    "\n",
    "\n",
    "#p1,m1=parser.encode_moves(heads,phrase,phrase)\n",
    "#p2,m2=parser.encode_moves(heads2,phrase2,phrase2)\n",
    "#print(m2)\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#p1 = torch.stack(p1, dim=0)\n",
    "#p2 = torch.stack(p2, dim=0)\n",
    "#print(p1)\n",
    "#print(p2)\n",
    "#input=pack_sequence([p1, p2])\n",
    "\n",
    "#input_size = 3840  # Each element in the sequence is a vector of size 2\n",
    "#hidden_size = 64\n",
    "#num_layers = 1\n",
    "#output_size = 3  # Example output size\n",
    "#model = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=False,proj_size=output_size)\n",
    "#model.half()\n",
    "#\n",
    "#output = model(input)\n",
    "#print(output)\n",
    "#print(\"OUTPUT\")\n",
    "#print(unpack_sequence(output[0]))\n",
    "\n",
    "\n",
    "oracle = Oracle()\n",
    "parser = Parser(oracle)\n",
    "create_batches(oracle,parser,100,train_prepocesed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadind 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:911: UserWarning: LSTM with projections is not supported with oneDNN. Using default implementation. (Triggered internally at ..\\aten\\src\\ATen\\native\\RNN.cpp:1474.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "input must have the type torch.float32, got type torch.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m oracle\u001b[38;5;241m.\u001b[39mtrain_on_batches()\n\u001b[0;32m      2\u001b[0m phrases\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHamad Butt è morto nel 1994 a 32 anni .\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m----> 3\u001b[0m deps \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparsing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrases\u001b[49m\u001b[43m,\u001b[49m\u001b[43mphrases\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(deps\u001b[38;5;241m.\u001b[39mget_heads())\n",
      "Cell \u001b[1;32mIn[15], line 191\u001b[0m, in \u001b[0;36mParser.parsing\u001b[1;34m(self, words, phrases_lemma)\u001b[0m\n\u001b[0;32m    189\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mextract_features(words,phrases_lemma,[stack],[i_buffer])\n\u001b[0;32m    190\u001b[0m features\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack(features)\n\u001b[1;32m--> 191\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mprint\u001b[39m(scores)\n\u001b[0;32m    193\u001b[0m scores\u001b[38;5;241m=\u001b[39mscores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "Cell \u001b[1;32mIn[15], line 89\u001b[0m, in \u001b[0;36mOracle.score\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m,features):\n\u001b[1;32m---> 89\u001b[0m     predicted_output,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_output\n",
      "File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:892\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    888\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[0;32m    889\u001b[0m                           max_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m    890\u001b[0m                           dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    891\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:821\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    817\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    818\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    819\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    820\u001b[0m                        ):\n\u001b[1;32m--> 821\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    823\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    825\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:234\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_is_any_autocast_enabled():\n\u001b[1;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have the type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    235\u001b[0m expected_input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m expected_input_dim:\n",
      "\u001b[1;31mValueError\u001b[0m: input must have the type torch.float32, got type torch.float64"
     ]
    }
   ],
   "source": [
    "\n",
    "oracle.train_on_batches()\n",
    "phrases=\"Hamad Butt è morto nel 1994 a 32 anni .\".split()\n",
    "deps = parser.parsing(phrases,phrases)\n",
    "print(deps.get_heads())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
