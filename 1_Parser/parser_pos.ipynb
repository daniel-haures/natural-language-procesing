{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44705,"status":"ok","timestamp":1715785018292,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"X_wq9I1GIEqz","outputId":"e39c6bac-69f5-4fb9-da71-9b51f29fdc05"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#/content/drive/MyDrive/batches\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","#/content/drive/MyDrive/batches"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15815,"status":"ok","timestamp":1715784869399,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"hQLM7wlvH_GT","outputId":"314432db-a628-418a-b565-d83563c10b12"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","cuda\n"]}],"source":["import pyconll\n","import pyconll.util\n","import torch\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","from torch.nn.utils.rnn import pack_sequence, unpack_sequence\n","import os\n","from sklearn import preprocessing\n","\n","#Set-up CUDA cores for training the model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9533,"status":"ok","timestamp":1715785035429,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"7jUj-grvH_GY"},"outputs":[],"source":["#Import and pre-proccess the training data with pyconll\n","train = pyconll.load_from_file('data/it_isdt-ud-train.conllu')\n","train_prepocesed=[]\n","for i,sent in enumerate(train):\n","    sentence_preprocesed=[]\n","    for j,token in enumerate(sent):\n","        if(token.head is not None):\n","            sentence_preprocesed.append(token)\n","    train_prepocesed.append(sentence_preprocesed)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":481,"status":"ok","timestamp":1715785839875,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"TO4-DLHkH_Gd"},"outputs":[],"source":["#Class of  objects used to store the output dependencies of the parser\n","class Dependencies(object):\n","    def __init__(self,n):\n","        self.n = n\n","        self.heads = [None] * (n+1)\n","        self.arcs = []\n","\n","    def get_heads(self):\n","        return self.heads\n","\n","    def add_arc(self, head, child):\n","        child=child\n","        self.heads[child]=head\n","        self.arcs.append((head,child))\n","\n","    def contains(self,head,child):\n","        child=child\n","        if self.heads[child]==head:\n","            return True\n","        else: return False\n","\n","\n","\n","\n","#Class of objects implementing the Berd Encoder and LSTM Oracle \n","class Oracle(object):\n","\n","    def __init__(self) -> None:\n","        #BERT encoder\n","        encoder_name = \"dbmdz/bert-base-italian-xxl-cased\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n","        self.encoder = AutoModel.from_pretrained(encoder_name)\n","\n","        #LSTM Oracle\n","        input_size = 4608 + 1200\n","        hidden_size = 512\n","        num_layers = 1\n","        output_size = 3\n","        self.epoch = 4\n","        self.model = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=False,proj_size=output_size).to(device)\n","\n","        #Loss criterion and Optimizer of the LSTM Oracle\n","        self.criterion = torch.nn.CrossEntropyLoss()\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0005)\n","\n","    #Compute the token ids used for each word by bert model\n","    def tokens(self,words,lemmas):\n","        words_token=[102]\n","        for i in range(0,len(lemmas)):\n","            word_token=self.tokenizer.convert_tokens_to_ids(words[i])\n","            lemma_token=self.tokenizer.convert_tokens_to_ids(lemmas[i])\n","            if(word_token!=101):\n","                words_token.append(word_token)\n","            else:\n","                words_token.append(lemma_token)\n","        words_token.append(103)\n","        return words_token\n","\n","    #Compute the contextualized embeddings of a sentence\n","    def encode(self,words,lemmas):\n","        words_token=self.tokens(words,lemmas)\n","        padded_input_ids = words_token\n","        input_tensor = torch.tensor([padded_input_ids])\n","        with torch.no_grad():\n","            outputs = self.encoder(input_tensor)\n","        return outputs.last_hidden_state\n","\n","    #Given a tensor describing the PoS of the stack and buffer, expand it's dimensionality in order to leverage the LSTM model.\n","    def expand_pos_tensor(self,sent_pos_tensor):\n","        extended_sent_pos_tensor=[]\n","        for move_pos_tensor in sent_pos_tensor:\n","            extended_move_pos_tensor=[]\n","            for token_pos in move_pos_tensor:\n","                extended_pos=torch.full((2,200),token_pos)[0]\n","                extended_move_pos_tensor.append(extended_pos)\n","            extended_move_pos_tensor=torch.cat(extended_move_pos_tensor,dim=0)\n","            extended_sent_pos_tensor.append(extended_move_pos_tensor)\n","        extended_sent_pos_tensor=torch.stack(extended_sent_pos_tensor)\n","        return extended_sent_pos_tensor\n","    \n","\n","    #Training the model with the 100 sentence batches stored in memory\n","    def train_on_batches(self):\n","        files = os.listdir(\"data/batches\")\n","        for k in range(0,self.epoch):\n","            for i in range(0,len(files)):\n","                if(i<200):\n","\n","                    batch_features,batch_upos,batch_moves=torch.load(f\"data/batches/tensor{i}.pt\")\n","                    batch_features=unpack_sequence(batch_features)\n","                    batch_upos=unpack_sequence(batch_upos)\n","                    batch_moves=unpack_sequence(batch_moves)\n","                    \n","                    for j in range(0,len(batch_moves)):\n","                        input_vector=torch.cat((batch_features[j],self.expand_pos_tensor(batch_upos[j])),dim=1)\n","                        predicted_output, _ = self.model(input_vector.to(device))\n","                        loss = self.criterion(predicted_output, batch_moves[j].to(device))\n","                        self.optimizer.zero_grad()\n","                        loss.backward()\n","                        self.optimizer.step()\n","\n","    #By PoS and the Embeddings of the words contained in the stack and the buffer, \n","    #outputs the probability vector of the three possible moves.\n","    def score(self,features,pos):\n","        input_vector=torch.cat((features,self.expand_pos_tensor(pos)),1)\n","        predicted_output,_ = self.model(input_vector.to(torch.float32).to(device))\n","        return predicted_output\n","\n","    #returns the last three element of stack\n","    def get_stack_context(self,list):\n","        depth=len(list)\n","\n","        if depth >= 3:\n","            return [list[-1], list[-2], list[-3]]\n","\n","        elif depth >= 2:\n","\n","            return [list[-1], list[-2], -1]\n","\n","        elif depth == 1:\n","            return [list[-1], -1 , -1]\n","        else:\n","            return [-1, -1, -1]\n","\n","    #returns the last three element of buffer\n","    def get_buffer_context(self,index,len_phrase):\n","        if(index==len_phrase-1):\n","            return [index,index+1,-1]\n","        elif(index==len_phrase):\n","            return [index,-1,-1]\n","        elif(index>len_phrase):\n","            return [-1,-1,-1]\n","        else: return [index,index+1,index+2]\n","\n","    \n","    \n","    #returns the embedded PoS of stack+buffer \n","    def extract_pos_features(self,phrases_pos,stacks,buffers):\n","        sent_pos_tensor=[]\n","        for i in range(0,len(stacks)):\n","            move_pos_tensor=[]\n","            stack_feature=self.get_stack_context(stacks[i])\n","            for el in stack_feature:\n","                if(el>=1):\n","                    move_pos_tensor.append((phrases_pos[el-1]+1)/10)\n","                elif(el==0):\n","                    move_pos_tensor.append(-0.1)\n","                else:\n","                    move_pos_tensor.append(0)\n","                    \n","            buffer_feature=self.get_buffer_context(buffers[i],len(phrases_pos))\n","            for el in buffer_feature:\n","                if(el>=1):\n","                    move_pos_tensor.append((phrases_pos[el-1]+1)/10)\n","                elif(el==0):\n","                    move_pos_tensor.append(-0.1)\n","                else:\n","                    move_pos_tensor.append(0)\n","            sent_pos_tensor.append(torch.tensor(move_pos_tensor))\n","        return sent_pos_tensor\n","    \n","    #given a matrix,it returns a tensor array\n","    def flatten_embedded_features(self,matrix):\n","        flat_list = torch.tensor([])\n","        for row in matrix:\n","            flat_list=torch.cat((flat_list,row))\n","        return flat_list\n","\n","    #returns the embedding of stack+buffer \n","    def extract_embedded_features(self,phrase,phrases_lemma,stacks,buffers):\n","        embeddings = self.encode(phrase,phrases_lemma)\n","        root_embeding=embeddings[0][0]\n","        empty_embedding=torch.tensor(np.zeros(768))\n","        embedded_features=[]\n","        for i in range(0,len(stacks)):\n","            stack_feature=self.get_stack_context(stacks[i])\n","            for j,el in enumerate(stack_feature):\n","                if(el>=1):\n","                    stack_feature[j]=embeddings[0][el]\n","                elif(el==0):\n","                    stack_feature[j]=root_embeding\n","                else:\n","                    stack_feature[j]=empty_embedding\n","            buffer_feature=self.get_buffer_context(buffers[i],len(phrase))\n","            for j,el in enumerate(buffer_feature):\n","                if(el>=1):\n","                    buffer_feature[j]=embeddings[0][el]\n","                else:\n","                    buffer_feature[j]=empty_embedding\n","            embedded_features.append(self.flatten_embedded_features(stack_feature+buffer_feature))\n","                    \n","        return embedded_features\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n","class Parser(object):\n","    def __init__(self,oracle):\n","        self.oracle=oracle\n","\n","    #applies the move, it goes to update the stack and the index of the buffer\n","    def transition(self,move, stack, i, dependencies):\n","        match move:\n","            case 0:\n","                stack.append(i)\n","                return stack,i+1,dependencies\n","            case 1:\n","                dependencies.add_arc(stack[-2], stack.pop())\n","                return stack,i,dependencies\n","            case 2:\n","                dependencies.add_arc(stack[-1], stack[-2])\n","                stack.pop(-2)\n","                return stack,i,dependencies\n","            case _:\n","                raise \"Wrong Move\"\n","\n","    #returns the possible moves that can be applied in parsing\n","    def get_valid_moves(self,i, n, stack_depth):\n","        moves = []\n","        if i <= n:\n","            moves.append(0)\n","        if stack_depth >= 2:\n","            moves.append(1)\n","            moves.append(2)\n","        return moves\n","    \n","    #given the sentence, the lemma and pos, It returns the depency related to that sentence\n","    def parsing(self,words,phrase_lemma,phrase_pos):\n","        n=len(words)\n","        deps=Dependencies(n)\n","        stack=[0]\n","        i_buffer=1\n","        moves=self.get_valid_moves(i_buffer,n,len(stack))\n","        old_stack=[]\n","        old_buffer=[]\n","        memory=1\n","        while moves:\n","            features_embeddings = self.oracle.extract_embedded_features(phrase_pos,[stack],[i_buffer])\n","            features_pos = self.oracle.extract_pos_features(words,phrase_lemma,[stack],[i_buffer])\n","\n","            features_embeddings=torch.stack(features_embeddings)\n","            features_pos=torch.stack(features_pos)\n","            \n","            scores = self.oracle.score(features_embeddings,features_pos)\n","            scores=scores[-1].tolist()\n","            \n","            next_move = max(moves, key=lambda move: scores[move])\n","            stack,i_buffer,deps = self.transition(next_move, stack, i_buffer, deps)\n","            moves = self.get_valid_moves(i_buffer,n,len(stack))\n","\n","            if(len(old_stack)<memory):\n","                old_stack.append(stack)\n","                old_buffer.append(i_buffer)\n","            else:\n","                old_stack.pop(0)\n","                old_buffer.pop(0)\n","                old_stack.append(stack)\n","                old_buffer.append(i_buffer)\n","\n","        return deps\n","\n","    #choose the best move for imulate_parse\n","    def check_best(self,heads,stack,buffer,deps,i):\n","        move=-1\n","        if(len(stack)>=2):\n","            children_list=[]\n","            for child,head in enumerate(heads):\n","                if head == stack[-1]:\n","                    children_list.append(child)\n","            if(heads[stack[-2]]==stack[-1]):\n","                move=2\n","            if(((heads[stack[-1]])==(stack[-2])) and all([deps.contains(stack[-1],child) for child in children_list])):\n","                move=1\n","        if(i<=len(buffer) and move==-1):\n","            move=0\n","        elif(i>len(buffer) and move==-1):\n","            move=None\n","        return move\n","    \n","    #it does reverse engineering, given the final state rebuilds the stack, buffer and moves\n","    def simulate_parse(self,heads,buffer):\n","        deps=Dependencies(len(buffer))\n","        stack=[0]\n","        moves=[]\n","        buffers=[]\n","        stacks=[]\n","        i=1\n","        best_move=self.check_best(heads,stack,buffer,deps,i)\n","        while best_move!=None:\n","            buffers.append(i)\n","            stacks.append(stack[:])\n","            moves.append(best_move)\n","            stack,i,deps=self.transition(best_move,stack,i,deps)\n","            best_move=self.check_best(heads,stack,buffer,deps,i)\n","        if(i>len(buffer)):\n","            return stacks,buffers,moves\n","        else: return None\n","\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"elapsed":3768,"status":"error","timestamp":1715785852266,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"9EEaWrtbH_Gh","outputId":"0666cf00-dd1c-4044-eb6c-46dc8e19c2de"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","Loadind 0\n","Loadind 1\n","Loadind 2\n","Loadind 3\n","Loadind 4\n","Loadind 5\n","Loadind 6\n","Loadind 7\n","Loadind 8\n","Loadind 9\n","Loadind 10\n","Loadind 11\n","Loadind 12\n","Loadind 13\n","Loadind 14\n","Loadind 15\n","Loadind 16\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m parser \u001b[38;5;241m=\u001b[39m Parser(oracle)\n\u001b[0;32m    110\u001b[0m create_batches(oracle,parser,\u001b[38;5;241m100\u001b[39m,train_prepocesed)\n\u001b[1;32m--> 111\u001b[0m \u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[18], line 102\u001b[0m, in \u001b[0;36mOracle.train_on_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m batch_moves\u001b[38;5;241m=\u001b[39munpack_sequence(batch_moves)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(batch_moves)):\n\u001b[1;32m--> 102\u001b[0m     input_vector\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((batch_features[j],\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_pos_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_upos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    103\u001b[0m     predicted_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_vector\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m    104\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(predicted_output, batch_moves[j]\u001b[38;5;241m.\u001b[39mto(device))\n","Cell \u001b[1;32mIn[18], line 76\u001b[0m, in \u001b[0;36mOracle.expand_pos_tensor\u001b[1;34m(self, sent_pos_tensor)\u001b[0m\n\u001b[0;32m     74\u001b[0m extended_move_pos_tensor\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token_pos \u001b[38;5;129;01min\u001b[39;00m move_pos_tensor:\n\u001b[1;32m---> 76\u001b[0m     extended_pos\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtoken_pos\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     77\u001b[0m     extended_move_pos_tensor\u001b[38;5;241m.\u001b[39mappend(extended_pos)\n\u001b[0;32m     78\u001b[0m extended_move_pos_tensor\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat(extended_move_pos_tensor,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["#given the phrase and its heads reconstructs the optimal moves to get those heads,also returns the embedding of the stack,buffer and pos\n","def encode_moves(oracle,parser,heads,phrase,phrase_lemma,phrase_pos):\n","    stacks,buffers,moves=parser.simulate_parse(heads,phrase)\n","\n","    embedded_features = oracle.oracle.extract_embedded_features(phrase_pos,stacks,buffers)\n","    pos_features = oracle.oracle.extract_pos_features(phrase_lemma,phrase_lemma,stacks,buffers)\n","\n","    expanded_moves=[]\n","    for move in moves:\n","        if(move==0): expanded_moves.append(torch.tensor([1,0,0]))\n","        if(move==1): expanded_moves.append(torch.tensor([0,1,0]))\n","        if(move==2): expanded_moves.append(torch.tensor([0,0,1]))\n","\n","    return embedded_features,pos_features,expanded_moves\n","\n","#takes the dataset and for each sentence generates the corresponding batch, will generate n=batch-size file\n","def create_batches(batch_size,dataset):\n","\n","    oracle = Oracle()\n","    parser = Parser(oracle)\n","    batch_feature=[]\n","    batch_pos=[]\n","    batch_moves=[]\n","\n","    pos_tags = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN','VERB','ADP','AUX','CCONJ','DET','NUM','PART','PRON','SCONJ','PUNCT','SYM','X']\n","    le = preprocessing.LabelEncoder()\n","    le = le.fit(pos_tags)\n","    \n","\n","    index_batch=0\n","    for sent in dataset:\n","        if(index_batch<200):\n","            heads=[-1]\n","            words=[]\n","            lemmas=[]\n","            pos=[]\n","\n","            wrong_sent=0\n","            for token in sent:\n","                if(token.head is None): wrong_sent=1\n","                if(token.form is None): wrong_sent=1\n","                if(token.lemma is None): wrong_sent=1\n","                if(token.upos is None): wrong_sent=1\n","\n","                heads.append(int(token.head))\n","                words.append(token.form)\n","                lemmas.append(token.lemma)\n","                pos.append(token.upos)\n","\n","            if(wrong_sent==0):\n","\n","                pos_int=le.transform(pos)\n","\n","                sent_features,sent_pos,sent_moves = encode_moves(oracle,parser,heads,words,lemmas,pos_int)\n","\n","                sent_features=torch.stack(sent_features,dim=0).to(torch.float32)\n","                sent_pos=torch.stack(sent_pos).to(torch.float32)\n","                sent_moves=torch.stack(sent_moves).to(torch.float32)\n","\n","            \n","\n","                batch_feature.append(sent_features)\n","                batch_pos.append(sent_pos)\n","                batch_moves.append(sent_moves)\n","                \n","\n","\n","            if(len(batch_moves)==batch_size):\n","                #print(sent_moves)\n","                print(index_batch)\n","                \n","                packed_features=pack_sequence(batch_feature,enforce_sorted=False)\n","                packed_pos=pack_sequence(batch_pos,enforce_sorted=False)\n","                packed_moves=pack_sequence(batch_moves,enforce_sorted=False)\n","                \n","\n","\n","                torch.save((packed_features,packed_pos,packed_moves), f\"data/batches/tensor{index_batch}.pt\")\n","                index_batch+=1\n","                batch_feature=[]\n","                batch_pos=[]\n","                batch_moves=[]\n","\n","\n","\n","#create_batches(100,train_prepocesed)\n","\n","\n","oracle = Oracle()\n","oracle.train_on_batches()\n","parser = Parser(oracle)\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFErqggjluhf","outputId":"ace05bfe-349a-4b4a-e1cd-490cbb4e6304"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]},{"ename":"ValueError","evalue":"input must have the type torch.float32, got type torch.float64","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m   accuracy\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m(total_wrong_tokens\u001b[38;5;241m/\u001b[39mtotal_tokens)\n\u001b[0;32m     60\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43maccuracy_on_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n","Cell \u001b[1;32mIn[17], line 46\u001b[0m, in \u001b[0;36maccuracy_on_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(wrong_sent\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     45\u001b[0m   pos_int\u001b[38;5;241m=\u001b[39mle\u001b[38;5;241m.\u001b[39mtransform(pos)\n\u001b[1;32m---> 46\u001b[0m   predicted_deps\u001b[38;5;241m=\u001b[39m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparsing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlemmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpos_int\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m   predicted_heads\u001b[38;5;241m=\u001b[39mpredicted_deps\u001b[38;5;241m.\u001b[39mget_heads()\n\u001b[0;32m     48\u001b[0m   sent_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(words)\n","Cell \u001b[1;32mIn[16], line 247\u001b[0m, in \u001b[0;36mParser.parsing\u001b[1;34m(self, words, phrase_lemma, phrase_pos)\u001b[0m\n\u001b[0;32m    244\u001b[0m features_embeddings\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack(features_embeddings)\n\u001b[0;32m    245\u001b[0m features_pos\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mstack(features_pos)\n\u001b[1;32m--> 247\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeatures_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m scores\u001b[38;5;241m=\u001b[39mscores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    250\u001b[0m next_move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(moves, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m move: scores[move])\n","Cell \u001b[1;32mIn[10], line 112\u001b[0m, in \u001b[0;36mOracle.score\u001b[1;34m(self, features, pos)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m,features,pos):\n\u001b[0;32m    111\u001b[0m     input_vector\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((features,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpand_pos_tensor(pos)),\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 112\u001b[0m     predicted_output,_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_output\n","File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:892\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    888\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[0;32m    889\u001b[0m                           max_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m    890\u001b[0m                           dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    891\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[1;32m--> 892\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n","File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:821\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    817\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    818\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    819\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    820\u001b[0m                        ):\n\u001b[1;32m--> 821\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    823\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    825\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[1;32mc:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:234\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_is_any_autocast_enabled():\n\u001b[1;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have the type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    235\u001b[0m expected_input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m expected_input_dim:\n","\u001b[1;31mValueError\u001b[0m: input must have the type torch.float32, got type torch.float64"]}],"source":["parser = Parser(oracle)\n","\n","def accuracy_on_test():\n","  #Load e preprocessing\n","  test = pyconll.load_from_file('data/it_isdt-ud-test.conllu')\n","  test_prepocesed=[]\n","  for i,sent in enumerate(test):\n","      sentence_preprocesed=[]\n","      for j,token in enumerate(sent):\n","          if(token.head is not None):\n","              sentence_preprocesed.append(token)\n","      test_prepocesed.append(sentence_preprocesed)\n","\n","  pos_tags = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN','VERB','ADP','AUX','CCONJ','DET','NUM','PART','PRON','SCONJ','PUNCT','SYM','X']\n","  le = preprocessing.LabelEncoder()\n","  le = le.fit(pos_tags)\n","\n","  #Accuracy\n","  total_tokens=0\n","  total_wrong_tokens=0\n","  for i,sent in enumerate(test):\n","    if(i<100):\n","      print(i)\n","      heads=[-1]\n","      words=[]\n","      lemmas=[]\n","      pos=[]\n","\n","      wrong_sent=0\n","      for token in sent:\n","        if(token.head is None): wrong_sent=1\n","        else:\n","          heads.append(int(token.head))\n","        if(token.form is None): wrong_sent=1\n","        if(token.lemma is None): wrong_sent=1\n","        if(token.upos is None): wrong_sent=1\n","\n","\n","\n","        words.append(token.form)\n","        lemmas.append(token.lemma)\n","        pos.append(token.upos)\n","\n","      if(wrong_sent!=1):\n","        pos_int=le.transform(pos)\n","        predicted_deps=parser.parsing(words,lemmas,pos_int)\n","        predicted_heads=predicted_deps.get_heads()\n","        sent_tokens=len(words)\n","        sent_wrong_tokens=0\n","        for j in range(1,len(predicted_heads)):\n","          if(predicted_heads[j] is not None):\n","            if(predicted_heads[j]!=heads[j]):\n","              sent_wrong_tokens+=1\n","          else:\n","            sent_wrong_tokens+=1\n","        total_tokens+=sent_tokens\n","        total_wrong_tokens+=sent_wrong_tokens\n","\n","  accuracy=1-(total_wrong_tokens/total_tokens)\n","  return accuracy\n","\n","print(accuracy_on_test())\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p8xFzbYo_UAH"},"source":["hidden 64, epoch 1, lr 0.001, memory 0, bidir False, batches 30 0.7\n","hidden 64, epoch 1, lr 0.01, memory 0, bidir False, batches 30 0.55 overfitting\n","hidden 64, epoch 1, lr 0.0001, memory 0, bidir False, batches 0.58 underfitting\n","\n","hidden 64, epoch 3, lr 0.001, memory 0, bidir False, batches 30 0.74\n","hidden 64, epoch 4, lr 0.001, memory 0, bidir False, batches 30 0.78 top\n","hidden 64, epoch 4, lr 0.001, memory 3, bidir False, batches 30 0.64 !\n","\n","hidden 64, epoch 4, lr 0.001, memory 0, bidir False, batches 60 0.75  overfitting\n","hidden 64, epoch 3, lr 0.001, memory 0, bidir False, batches 60 0.76  overfitting\n","hidden 64, epoch 2, lr 0.001, memory 0, bidir False, batches 60 0.78  uguale a prima\n","hidden 64, epoch 1, lr 0.001, memory 0, bidir False, batches 60 0.71  underfitting\n","\n","hidden 64, epoch 2, lr 0.0005, memory 0, bidir False, batches 120 0.78  \n","hidden 64, epoch 3, lr 0.0005, memory 0, bidir False, batches 120 0.78  \n","hidden 128, epoch 3, lr 0.0005, memory 0, bidir False, batches 120 0.815  top\n","hidden 512, epoch 4, lr 0.0005, memory 0, bidir False, batches 120 0.83  top"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
