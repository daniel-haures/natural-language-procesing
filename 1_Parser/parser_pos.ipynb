{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44705,"status":"ok","timestamp":1715785018292,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"X_wq9I1GIEqz","outputId":"e39c6bac-69f5-4fb9-da71-9b51f29fdc05"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#/content/drive/MyDrive/batches\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","#/content/drive/MyDrive/batches"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15815,"status":"ok","timestamp":1715784869399,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"hQLM7wlvH_GT","outputId":"314432db-a628-418a-b565-d83563c10b12"},"outputs":[{"name":"stdout","output_type":"stream","text":["False\n","cpu\n"]}],"source":["#from libs.corpus import openConllu, check_projectivity\n","import pyconll\n","import pyconll.util\n","import torch\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","from torch.nn.utils.rnn import pack_sequence, unpack_sequence\n","import os\n","from sklearn import preprocessing\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print( torch.cuda.is_available())\n","print(device)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9533,"status":"ok","timestamp":1715785035429,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"7jUj-grvH_GY"},"outputs":[],"source":["\n","train = pyconll.load_from_file('data/it_isdt-ud-train.conllu')\n","train_prepocesed=[]\n","for i,sent in enumerate(train):\n","    sentence_preprocesed=[]\n","    for j,token in enumerate(sent):\n","        if(token.head is not None):\n","            sentence_preprocesed.append(token)\n","    train_prepocesed.append(sentence_preprocesed)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqqytbPvwMzF"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":481,"status":"ok","timestamp":1715785839875,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"TO4-DLHkH_Gd"},"outputs":[],"source":["\n","\n","\n","\n","\n","#0:SHIFT 1:RIGHTARC 2:LEFTARC\n","moves = [0, 1, 2]\n","\n","\n","#creiamo un oggetto Dependencies per salvare le dependencies\n","class Dependencies(object):\n","    def __init__(self,n):\n","        self.n = n\n","        self.heads = [None] * (n+1)\n","        self.arcs = []\n","\n","    def get_heads(self):\n","        return self.heads\n","\n","    def add_arc(self, head, child):\n","        child=child\n","        self.heads[child]=head\n","        self.arcs.append((head,child))\n","\n","    def contains(self,head,child):\n","        child=child\n","        if self.heads[child]==head:\n","            return True\n","        else: return False\n","\n","\n","\n","\n","\n","class Oracle(object):\n","\n","    def __init__(self) -> None:\n","        #BERT encoder\n","        encoder_name = \"dbmdz/bert-base-italian-xxl-cased\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n","        self.encoder = AutoModel.from_pretrained(encoder_name)\n","\n","        #LSTM oracle\n","        input_size = 4608 + 1200\n","        hidden_size = 64\n","        num_layers = 1\n","        output_size = 3\n","        self.epoch = 1\n","        self.model = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=False,proj_size=output_size).to(device)\n","\n","        #Oracle critenion and optimazation\n","        self.criterion = torch.nn.CrossEntropyLoss()\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n","\n","    def tokens(self,words,lemmas):\n","        words_token=[102]\n","        for i in range(0,len(lemmas)):\n","            word_token=self.tokenizer.convert_tokens_to_ids(words[i])\n","            lemma_token=self.tokenizer.convert_tokens_to_ids(lemmas[i])\n","            if(word_token!=101):\n","                words_token.append(word_token)\n","            else:\n","                words_token.append(lemma_token)\n","        words_token.append(103)\n","        return words_token\n","\n","    def encode(self,words,lemmas):\n","        words_token=self.tokens(words,lemmas)\n","        #max_length = 64  # Example max length ????\n","        padded_input_ids = words_token\n","        input_tensor = torch.tensor([padded_input_ids])\n","        with torch.no_grad():\n","            outputs = self.encoder(input_tensor)\n","        return outputs.last_hidden_state\n","\n","    def expand_pos_tensor(self,sent_pos_tensor):\n","        extended_sent_pos_tensor=[]\n","        for move_pos_tensor in sent_pos_tensor:\n","            extended_move_pos_tensor=[]\n","            for token_pos in move_pos_tensor:\n","                extended_pos=torch.full((2,200),token_pos)[0]\n","                extended_move_pos_tensor.append(extended_pos)\n","            extended_move_pos_tensor=torch.cat(extended_move_pos_tensor,dim=0)\n","            extended_sent_pos_tensor.append(extended_move_pos_tensor)\n","        extended_sent_pos_tensor=torch.stack(extended_sent_pos_tensor)\n","        return extended_sent_pos_tensor\n","    \n","\n","          \n","\n","\n","    #prende i batches dalla cartella, e allena il modello\n","    def train_on_batches(self):\n","        files = os.listdir(\"data/batches\")\n","        for k in range(0,self.epoch):\n","          for i in range(0,len(files)):\n","              if(i==0):\n","\n","                  print(\"Loadind \"+str(i))\n","                  \n","                  batch_features,batch_upos,batch_moves=torch.load(f\"data/batches/tensor{i}.pt\")\n","                  batch_features=unpack_sequence(batch_features)\n","                  batch_upos=unpack_sequence(batch_upos)\n","                  batch_moves=unpack_sequence(batch_moves)\n","                  \n","                  for j in range(0,len(batch_moves)):\n","                      input_vector=torch.cat((batch_features[j],self.expand_pos_tensor(batch_upos[j])),dim=1)\n","                      predicted_output, _ = self.model(input_vector.to(device))\n","                      loss = self.criterion(predicted_output, batch_moves[j].to(device))\n","                      self.optimizer.zero_grad()\n","                      loss.backward()\n","                      self.optimizer.step()\n","\n","\n","    def score(self,features,pos):\n","        input_vector=torch.cat((features,self.expand_pos_tensor(pos)),1)\n","        predicted_output,_ = self.model(input_vector.to(device))\n","        return predicted_output\n","\n","    #prende gli ultimi tre elementi dello stack\n","    def get_stack_context(self,list):\n","        depth=len(list)\n","\n","        if depth >= 3:\n","            return [list[-1], list[-2], list[-3]]\n","\n","        elif depth >= 2:\n","\n","            return [list[-1], list[-2], -1]\n","\n","        elif depth == 1:\n","            return [list[-1], -1 , -1]\n","        else:\n","            return [-1, -1, -1]\n","\n","    #prende gli ultimi due elementi dell buffer\n","    def get_buffer_context(self,index,len_phrase):\n","        if(index==len_phrase-1):\n","            return [index,index+1,-1]\n","        elif(index==len_phrase):\n","            return [index,-1,-1]\n","        elif(index>len_phrase):\n","            return [-1,-1,-1]\n","        else: return [index,index+1,index+2]\n","\n","    def flatten_embedded_features(self,matrix):\n","        flat_list = torch.tensor([])\n","        for row in matrix:\n","            flat_list=torch.cat((flat_list,row))\n","        return flat_list\n","    \n","    def extract_pos_features(self,phrases_pos,stacks,buffers):\n","        sent_pos_tensor=[]\n","        for i in range(0,len(stacks)):\n","            move_pos_tensor=[]\n","            stack_feature=self.get_stack_context(stacks[i])\n","            for el in stack_feature:\n","                if(el>=1):\n","                    move_pos_tensor.append((phrases_pos[el-1]+1)/10)\n","                elif(el==0):\n","                    move_pos_tensor.append(-0.1)\n","                else:\n","                    move_pos_tensor.append(0)\n","                    \n","            buffer_feature=self.get_buffer_context(buffers[i],len(phrases_pos))\n","            for el in buffer_feature:\n","                if(el>=1):\n","                    move_pos_tensor.append((phrases_pos[el-1]+1)/10)\n","                elif(el==0):\n","                    move_pos_tensor.append(-0.1)\n","                else:\n","                    move_pos_tensor.append(0)\n","            sent_pos_tensor.append(torch.tensor(move_pos_tensor))\n","        return sent_pos_tensor\n","\n","\n","    def extract_features(self,phrase,phrases_lemma,phrase_pos,stacks,buffers):\n","        embeddings = self.encode(phrase,phrases_lemma)\n","        root_embeding=embeddings[0][0]\n","        empty_embedding=torch.tensor(np.zeros(768))\n","        embedded_features=[]\n","        for i in range(0,len(stacks)):\n","            stack_feature=self.get_stack_context(stacks[i])\n","            for j,el in enumerate(stack_feature):\n","                if(el>=1):\n","                    stack_feature[j]=embeddings[0][el]\n","                elif(el==0):\n","                    stack_feature[j]=root_embeding\n","                else:\n","                    stack_feature[j]=empty_embedding\n","            buffer_feature=self.get_buffer_context(buffers[i],len(phrase))\n","            for j,el in enumerate(buffer_feature):\n","                if(el>=1):\n","                    buffer_feature[j]=embeddings[0][el]\n","                else:\n","                    buffer_feature[j]=empty_embedding\n","            embedded_features.append(self.flatten_embedded_features(stack_feature+buffer_feature))\n","\n","        pos_features=self.extract_pos_features(phrase_pos,stacks,buffers)\n","        \n","        return embedded_features, pos_features\n","    \n","\n","\n","\n","\n","\n","class Parser(object):\n","    def __init__(self,oracle):\n","        self.oracle=oracle\n","\n","    #applica la mossa andando ad aggiornare lo stack e l'indice del buffer\n","    def transition(self,move, stack, i, dependencies):\n","        match move:\n","            case 0:\n","                stack.append(i)\n","                return stack,i+1,dependencies\n","            case 1:\n","                dependencies.add_arc(stack[-2], stack.pop())\n","                return stack,i,dependencies\n","            case 2:\n","                dependencies.add_arc(stack[-1], stack[-2])\n","                stack.pop(-2)\n","                return stack,i,dependencies\n","            case _:\n","                raise \"Wrong Move\"\n","\n","    #ritorna le mosse possibili che si possono applicare\n","    def get_valid_moves(self,i, n, stack_depth):\n","        moves = []\n","        if i <= n:\n","            moves.append(0)\n","        if stack_depth >= 2:\n","            moves.append(1)\n","            moves.append(2)\n","        return moves\n","\n","    def parsing(self,words,phrase_lemma,phrase_pos):\n","        n=len(words)\n","        deps=Dependencies(n)\n","        stack=[0]\n","        i_buffer=1\n","        moves=self.get_valid_moves(i_buffer,n,len(stack))\n","        old_stack=[]\n","        old_buffer=[]\n","        memory=1\n","        while moves:\n","            features_embeddings,features_pos = self.oracle.extract_features(words,phrase_lemma,phrase_pos[stack],[i_buffer])\n","            features_embeddings=torch.stack(features_embeddings)\n","            features_pos=torch.stack(features_pos)\n","            \n","            scores = self.oracle.score(features_embeddings,features_pos)\n","            scores=scores[-1].tolist()\n","            \n","            next_move = max(moves, key=lambda move: scores[move])\n","            stack,i_buffer,deps = self.transition(next_move, stack, i_buffer, deps)\n","            moves = self.get_valid_moves(i_buffer,n,len(stack))\n","\n","            if(len(old_stack)<memory):\n","                old_stack.append(stack)\n","                old_buffer.append(i_buffer)\n","            else:\n","                old_stack.pop(0)\n","                old_buffer.pop(0)\n","                old_stack.append(stack)\n","                old_buffer.append(i_buffer)\n","\n","        return deps\n","\n","    #sceglie la mossa migliore da eseguire nel simulate_parse\n","    def check_best(self,heads,stack,buffer,deps,i):\n","        move=-1\n","        if(len(stack)>=2):\n","            children_list=[]\n","            for child,head in enumerate(heads):\n","                if head == stack[-1]:\n","                    children_list.append(child)\n","            if(heads[stack[-2]]==stack[-1]):\n","                move=2\n","            if(((heads[stack[-1]])==(stack[-2])) and all([deps.contains(stack[-1],child) for child in children_list])):\n","                move=1\n","        if(i<=len(buffer) and move==-1):\n","            move=0\n","        elif(i>len(buffer) and move==-1):\n","            move=None\n","        return move\n","    #fa reverse engineering, dato lo stato finale ricostruisce lo stack, buffer e le mosse\n","    def simulate_parse(self,heads,buffer):\n","        deps=Dependencies(len(buffer))\n","        stack=[0]\n","        moves=[]\n","        buffers=[]\n","        stacks=[]\n","        i=1\n","        best_move=self.check_best(heads,stack,buffer,deps,i)\n","        while best_move!=None:\n","            buffers.append(i)\n","            stacks.append(stack[:])\n","            moves.append(best_move)\n","            stack,i,deps=self.transition(best_move,stack,i,deps)\n","            best_move=self.check_best(heads,stack,buffer,deps,i)\n","        if(i>len(buffer)):\n","            return stacks,buffers,moves\n","        else: return None\n","\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"elapsed":3768,"status":"error","timestamp":1715785852266,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"9EEaWrtbH_Gh","outputId":"0666cf00-dd1c-4044-eb6c-46dc8e19c2de"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","Loadind 0\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\danie\\.venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:911: UserWarning: LSTM with projections is not supported with oneDNN. Using default implementation. (Triggered internally at ..\\aten\\src\\ATen\\native\\RNN.cpp:1474.)\n","  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"]},{"name":"stdout","output_type":"stream","text":["Loadind 0\n","Loadind 0\n","Loadind 0\n"]}],"source":["def encode_moves(oracle,parser,heads,phrase,phrase_lemma,phrase_pos):\n","    stacks,buffers,moves=parser.simulate_parse(heads,phrase)\n","    embedded_features, pos_features=oracle.extract_features(phrase,phrase_lemma,phrase_pos,stacks,buffers)\n","\n","\n","    expanded_moves=[]\n","    for move in moves:\n","        if(move==0): expanded_moves.append(torch.tensor([1,0,0]))\n","        if(move==1): expanded_moves.append(torch.tensor([0,1,0]))\n","        if(move==2): expanded_moves.append(torch.tensor([0,0,1]))\n","\n","    return embedded_features,pos_features,expanded_moves\n","\n","#prende il dataset e per ogni frase genera il batch corrispettivo, genererà n=batch-size file\n","def create_batches(oracle,parser,batch_size,dataset):\n","    batch_feature=[]\n","    batch_pos=[]\n","    batch_moves=[]\n","\n","    pos_tags = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN','VERB','ADP','AUX','CCONJ','DET','NUM','PART','PRON','SCONJ','PUNCT','SYM','X']\n","    le = preprocessing.LabelEncoder()\n","    le = le.fit(pos_tags)\n","    \n","\n","    index_batch=0\n","    for sent in dataset:\n","        if(index_batch<2):\n","            heads=[-1]\n","            words=[]\n","            lemmas=[]\n","            pos=[]\n","\n","            wrong_sent=0\n","            for token in sent:\n","                if(token.head is None): wrong_sent=1\n","                if(token.form is None): wrong_sent=1\n","                if(token.lemma is None): wrong_sent=1\n","                if(token.upos is None): wrong_sent=1\n","\n","                heads.append(int(token.head))\n","                words.append(token.form)\n","                lemmas.append(token.lemma)\n","                pos.append(token.upos)\n","\n","            if(wrong_sent==0):\n","\n","                pos_int=le.transform(pos)\n","\n","                sent_features,sent_pos,sent_moves = encode_moves(oracle,parser,heads,words,lemmas,pos_int)\n","\n","                sent_features=torch.stack(sent_features,dim=0).to(torch.float32)\n","                sent_pos=torch.stack(sent_pos).to(torch.float32)\n","                sent_moves=torch.stack(sent_moves).to(torch.float32)\n","\n","                batch_feature.append(sent_features)\n","                batch_pos.append(sent_pos)\n","                batch_moves.append(sent_moves)\n","                \n","\n","\n","            if(len(batch_moves)==batch_size):\n","                #print(sent_moves)\n","                print(index_batch)\n","                packed_features=pack_sequence(batch_feature,enforce_sorted=False)\n","                packed_pos=pack_sequence(batch_pos,enforce_sorted=False)\n","                packed_moves=pack_sequence(batch_moves,enforce_sorted=False)\n","                \n","\n","\n","                torch.save((packed_features,packed_pos,packed_moves), f\"data/batches/tensor{index_batch}.pt\")\n","                index_batch+=1\n","                batch_feature=[]\n","                batch_pos=[]\n","                batch_moves=[]\n","#DA FARE:\n","#1. Salvare batch da 50-100 frasi su file dati, ossia per ogni frase gli stati con relativi stack,buffer e move.\n","#2. Importare batch per batch come nell'esempio di chat gpt.\n","#3. Per ogni batch fare forward e back-prop di adam optimizer come nell'esempio di chat_gpt.\n","\n","\n","\n","#p1,m1=parser.encode_moves(heads,phrase,phrase)\n","#p2,m2=parser.encode_moves(heads2,phrase2,phrase2)\n","#print(m2)\n","#torch.set_printoptions(profile=\"full\")\n","#p1 = torch.stack(p1, dim=0)\n","#p2 = torch.stack(p2, dim=0)\n","#print(p1)\n","#print(p2)\n","#input=pack_sequence([p1, p2])\n","\n","#input_size = 3840  # Each element in the sequence is a vector of size 2\n","#hidden_size = 64\n","#num_layers = 1\n","#output_size = 3  # Example output size\n","#model = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=False,proj_size=output_size)\n","#model.half()\n","#\n","#output = model(input)\n","#print(output)\n","#print(\"OUTPUT\")\n","#print(unpack_sequence(output[0]))\n","\n","\n","oracle = Oracle()\n","parser = Parser(oracle)\n","create_batches(oracle,parser,100,train_prepocesed)\n","oracle.train_on_batches()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2326,"status":"ok","timestamp":1715782785284,"user":{"displayName":"Lorenzo Giacometti","userId":"13895654272715125957"},"user_tz":-120},"id":"zik-41O6H_Gj","outputId":"5f65f734-32fd-40e3-d964-04c81731d29b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[None, 2, 4, 4, 0, 6, 4, 8, 10, 10, 4, 12, 10, 4]\n","[[0], [0, 1], [0, 1, 2], [0, 2], [0, 2, 3], [0, 2, 3, 4], [0, 2, 4], [0, 4], [0, 4, 5], [0, 4, 5, 6], [0, 4, 6], [0, 4], [0, 4, 7], [0, 4, 7, 8], [0, 4, 8], [0, 4, 8, 9], [0, 4, 8, 9, 10], [0, 4, 8, 10], [0, 4, 8, 10, 11], [0, 4, 8, 10, 11, 12], [0, 4, 8, 10, 12], [0, 4, 8, 10], [0, 4, 8], [0, 4], [0, 4, 13], [0, 4]]\n","[0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 0, 2, 1, 1, 1, 0, 1, 1]\n"]}],"source":["\n","\n","parser = Parser(oracle)\n","phrases1= \"Si chiamava Prototaxites .\".split()\n","phrases2= \"In la mia mente , almeno , è così .\".split()\n","phrases3=\" I turisti possono usare le barche per visitare alcune zone di Roma .\".split()\n","phrases4=\"Il più largo organismo a il mondo è un tappeto miceliale , di lo spessore di una parete cellulare .\".split()\n","heads1=[-1,2,0,2,2]\n","heads2=[-1,4,4,4,9,4,4,6,9,0,9]\n","heads3=[-1,2,4,4,0,6,4,8,4,10,8,12,10,4]\n","heads4=[-1,4,3,4,0,7,7,4,4,10,4,4,4,15,15,4,18,18,15,18,4]\n","deps = parser.parsing(phrases3,phrases3)\n","print(deps.get_heads())\n","stacks,buffers,moves=parser.simulate_parse(heads3,phrases3)\n","print(stacks)\n","print(moves)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFErqggjluhf","outputId":"ace05bfe-349a-4b4a-e1cd-490cbb4e6304"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n"]}],"source":["def accuracy_on_test():\n","  #Load e preprocessing\n","  test = pyconll.load_from_file('/content/drive/MyDrive/magistrale/TLN/data/it_isdt-ud-test.conllu')\n","  test_prepocesed=[]\n","  for i,sent in enumerate(test):\n","      sentence_preprocesed=[]\n","      for j,token in enumerate(sent):\n","          if(token.head is not None):\n","              sentence_preprocesed.append(token)\n","      test_prepocesed.append(sentence_preprocesed)\n","\n","  #Accuracy\n","  total_tokens=0\n","  total_wrong_tokens=0\n","  for i,sent in enumerate(test):\n","    if(i<100):\n","      print(i)\n","      heads=[-1]\n","      words=[]\n","      lemmas=[]\n","\n","      wrong_sent=0\n","      for token in sent:\n","        if(token.head is None): wrong_sent=1\n","        else:\n","          heads.append(int(token.head))\n","        if(token.form is None): wrong_sent=1\n","        if(token.lemma is None): wrong_sent=1\n","\n","\n","        words.append(token.form)\n","        lemmas.append(token.lemma)\n","\n","      if(wrong_sent!=1):\n","        predicted_deps=parser.parsing(words,lemmas)\n","        predicted_heads=predicted_deps.get_heads()\n","        sent_tokens=len(words)\n","        sent_wrong_tokens=0\n","        for j in range(1,len(predicted_heads)):\n","          if(predicted_heads[j] is not None):\n","            if(predicted_heads[j]!=heads[j]):\n","              sent_wrong_tokens+=1\n","          else:\n","            sent_wrong_tokens+=1\n","        total_tokens+=sent_tokens\n","        total_wrong_tokens+=sent_wrong_tokens\n","\n","  accuracy=1-(total_wrong_tokens/total_tokens)\n","  return accuracy\n","\n","print(accuracy_on_test())\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p8xFzbYo_UAH"},"source":["hidden 64, epoch 1, lr 0.001, memory 0, bidir False, batches 30 0.7\n","hidden 64, epoch 1, lr 0.01, memory 0, bidir False, batches 30 0.55 overfitting\n","hidden 64, epoch 1, lr 0.0001, memory 0, bidir False, batches 0.58 underfitting\n","\n","hidden 64, epoch 3, lr 0.001, memory 0, bidir False, batches 30 0.74\n","hidden 64, epoch 4, lr 0.001, memory 0, bidir False, batches 30 0.78 top\n","hidden 64, epoch 4, lr 0.001, memory 3, bidir False, batches 30 0.64 !\n","\n","hidden 64, epoch 4, lr 0.001, memory 0, bidir False, batches 60 0.75  overfitting\n","hidden 64, epoch 3, lr 0.001, memory 0, bidir False, batches 60 0.76  overfitting\n","hidden 64, epoch 2, lr 0.001, memory 0, bidir False, batches 60 0.78  uguale a prima\n","hidden 64, epoch 1, lr 0.001, memory 0, bidir False, batches 60 0.71  underfitting\n","\n","hidden 64, epoch 2, lr 0.0005, memory 0, bidir False, batches 120 0.78  \n","hidden 64, epoch 3, lr 0.0005, memory 0, bidir False, batches 120 0.78  \n","hidden 128, epoch 3, lr 0.0005, memory 0, bidir False, batches 120 0.815  top\n","hidden 512, epoch 4, lr 0.0005, memory 0, bidir False, batches 120 0.83  top"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
