{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15815,"status":"ok","timestamp":1715784869399,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"hQLM7wlvH_GT","outputId":"314432db-a628-418a-b565-d83563c10b12"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import pyconll\n","import pyconll.util\n","import torch\n","from transformers import AutoModel, AutoTokenizer\n","import numpy as np\n","from torch.nn.utils.rnn import pack_sequence, unpack_sequence\n","import os\n","from sklearn import preprocessing\n","\n","#Set-up CUDA cores for training the model\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":9533,"status":"ok","timestamp":1715785035429,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"7jUj-grvH_GY"},"outputs":[],"source":["#Import and pre-proccess the training data with pyconll\n","train = pyconll.load_from_file('data/it_isdt-ud-train.conllu')\n","train_prepocesed=[]\n","for i,sent in enumerate(train):\n","    sentence_preprocesed=[]\n","    for j,token in enumerate(sent):\n","        if(token.head is not None):\n","            sentence_preprocesed.append(token)\n","    train_prepocesed.append(sentence_preprocesed)\n","\n","dev = pyconll.load_from_file('data/it_isdt-ud-dev.conllu')\n","dev_prepocesed=[]\n","for i,sent in enumerate(dev):\n","    sentence_preprocesed=[]\n","    for j,token in enumerate(sent):\n","        if(token.head is not None):\n","            sentence_preprocesed.append(token)\n","    dev_prepocesed.append(sentence_preprocesed)\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":481,"status":"ok","timestamp":1715785839875,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"TO4-DLHkH_Gd"},"outputs":[],"source":["#Class of  objects used to store the output dependencies of the parser\n","class Dependencies(object):\n","    def __init__(self,n):\n","        self.n = n\n","        self.heads = [None] * (n+1)\n","        self.arcs = []\n","\n","    def get_heads(self):\n","        return self.heads\n","\n","    def add_arc(self, head, child):\n","        child=child\n","        self.heads[child]=head\n","        self.arcs.append((head,child))\n","\n","    def contains(self,head,child):\n","        child=child\n","        if self.heads[child]==head:\n","            return True\n","        else: return False\n","\n","\n","#Class of objects implementing the Berd Encoder and LSTM Oracle \n","class Oracle(object):\n","\n","    def __init__(self) -> None:\n","        #BERT encoder\n","        encoder_name = \"dbmdz/bert-base-italian-xxl-cased\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n","        self.encoder = AutoModel.from_pretrained(encoder_name)\n","\n","        #LSTM Oracle\n","        input_size = 4608 + 1200\n","        hidden_size = 512\n","        num_layers = 1\n","        output_size = 3\n","        self.epoch = 4\n","        self.model = torch.nn.LSTM(input_size,hidden_size,num_layers,batch_first=True,bidirectional=False,proj_size=output_size).to(device)\n","\n","        #Loss criterion and Optimizer of the LSTM Oracle\n","        self.criterion = torch.nn.CrossEntropyLoss()\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0005)\n","\n","    #Compute the token ids used for each word by bert model\n","    def tokens(self,words,lemmas):\n","        words_token=[102]\n","        for i in range(0,len(lemmas)):\n","            word_token=self.tokenizer.convert_tokens_to_ids(words[i])\n","            lemma_token=self.tokenizer.convert_tokens_to_ids(lemmas[i])\n","            if(word_token!=101):\n","                words_token.append(word_token)\n","            else:\n","                words_token.append(lemma_token)\n","        words_token.append(103)\n","        return words_token\n","\n","    #Compute the contextualized embeddings of a sentence\n","    def encode(self,words,lemmas):\n","        words_token=self.tokens(words,lemmas)\n","        padded_input_ids = words_token\n","        input_tensor = torch.tensor([padded_input_ids])\n","        with torch.no_grad():\n","            outputs = self.encoder(input_tensor)\n","        return outputs.last_hidden_state\n","\n","    #Given a tensor describing the PoS of the stack and buffer, expand it's dimensionality in order to leverage the LSTM model.\n","    def expand_pos_tensor(self,sent_pos_tensor):\n","        extended_sent_pos_tensor=[]\n","        for move_pos_tensor in sent_pos_tensor:\n","            extended_move_pos_tensor=[]\n","            for token_pos in move_pos_tensor:\n","                extended_pos=torch.full((2,200),token_pos)[0]\n","                extended_move_pos_tensor.append(extended_pos)\n","            extended_move_pos_tensor=torch.cat(extended_move_pos_tensor,dim=0)\n","            extended_sent_pos_tensor.append(extended_move_pos_tensor)\n","        extended_sent_pos_tensor=torch.stack(extended_sent_pos_tensor)\n","        return extended_sent_pos_tensor\n","    \n","\n","    #Training the model with n sentence batches stored in memory\n","    def train_on_batches(self):\n","        files = os.listdir(\"data/batches\")\n","        for k in range(0,self.epoch):\n","            for i in range(0,len(files)):\n","                if(i<200):\n","\n","                    batch_features,batch_upos,batch_moves=torch.load(f\"data/batches/tensor{i}.pt\")\n","                    batch_features=unpack_sequence(batch_features)\n","                    batch_upos=unpack_sequence(batch_upos)\n","                    batch_moves=unpack_sequence(batch_moves)\n","                    \n","                    for j in range(0,len(batch_moves)):\n","                        input_vector=torch.cat((batch_features[j],self.expand_pos_tensor(batch_upos[j])),dim=1)\n","                        predicted_output, _ = self.model(input_vector.to(device))\n","                        loss = self.criterion(predicted_output, batch_moves[j].to(device))\n","                        self.optimizer.zero_grad()\n","                        loss.backward()\n","                        self.optimizer.step()\n","\n","    #By PoS and the Embeddings of the words contained in the stack and the buffer, \n","    #outputs the probability vector of the three possible moves.\n","    def score(self,features,pos):\n","        input_vector=torch.cat((features,self.expand_pos_tensor(pos)),1)\n","        predicted_output,_ = self.model(input_vector.to(torch.float32).to(device))\n","        return predicted_output\n","\n","    #returns the last three element of stack\n","    def get_stack_context(self,list):\n","        depth=len(list)\n","\n","        if depth >= 3:\n","            return [list[-1], list[-2], list[-3]]\n","\n","        elif depth >= 2:\n","\n","            return [list[-1], list[-2], -1]\n","\n","        elif depth == 1:\n","            return [list[-1], -1 , -1]\n","        else:\n","            return [-1, -1, -1]\n","\n","    #returns the last three element of buffer\n","    def get_buffer_context(self,index,len_phrase):\n","        if(index==len_phrase-1):\n","            return [index,index+1,-1]\n","        elif(index==len_phrase):\n","            return [index,-1,-1]\n","        elif(index>len_phrase):\n","            return [-1,-1,-1]\n","        else: return [index,index+1,index+2]\n","\n","    \n","    #returns the embedded PoS of stack+buffer \n","    def extract_pos_features(self,phrases_pos,stacks,buffers):\n","        sent_pos_tensor=[]\n","        for i in range(0,len(stacks)):\n","            move_pos_tensor=[]\n","            stack_feature=self.get_stack_context(stacks[i])\n","            for el in stack_feature:\n","                if(el>=1):\n","                    move_pos_tensor.append((phrases_pos[el-1]+1)/10)\n","                elif(el==0):\n","                    move_pos_tensor.append(-0.1)\n","                else:\n","                    move_pos_tensor.append(0)\n","                    \n","            buffer_feature=self.get_buffer_context(buffers[i],len(phrases_pos))\n","            for el in buffer_feature:\n","                if(el>=1):\n","                    move_pos_tensor.append((phrases_pos[el-1]+1)/10)\n","                elif(el==0):\n","                    move_pos_tensor.append(-0.1)\n","                else:\n","                    move_pos_tensor.append(0)\n","            sent_pos_tensor.append(torch.tensor(move_pos_tensor))\n","        return sent_pos_tensor\n","    \n","    #given a matrix,it returns a tensor array\n","    def flatten_embedded_features(self,matrix):\n","        flat_list = torch.tensor([])\n","        for row in matrix:\n","            flat_list=torch.cat((flat_list,row))\n","        return flat_list\n","\n","    #returns the embedding of stack+buffer \n","    def extract_embedded_features(self,embeddings,len_sent,stacks,buffers):\n","        root_embeding=embeddings[0][0]\n","        empty_embedding=torch.tensor(np.zeros(768))\n","        embedded_features=[]\n","        for i in range(0,len(stacks)):\n","            stack_feature=self.get_stack_context(stacks[i])\n","            for j,el in enumerate(stack_feature):\n","                if(el>=1):\n","                    stack_feature[j]=embeddings[0][el]\n","                elif(el==0):\n","                    stack_feature[j]=root_embeding\n","                else:\n","                    stack_feature[j]=empty_embedding\n","            buffer_feature=self.get_buffer_context(buffers[i],len_sent)\n","            for j,el in enumerate(buffer_feature):\n","                if(el>=1):\n","                    buffer_feature[j]=embeddings[0][el]\n","                else:\n","                    buffer_feature[j]=empty_embedding\n","            embedded_features.append(self.flatten_embedded_features(stack_feature+buffer_feature))\n","                    \n","        return embedded_features\n","    "]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["\n","class Parser(object):\n","    def __init__(self,oracle):\n","        self.oracle=oracle\n","\n","    #applies the move, it goes to update the stack and the index of the buffer\n","    def transition(self,move, stack, i, dependencies):\n","        match move:\n","            case 0:\n","                stack.append(i)\n","                return stack,i+1,dependencies\n","            case 1:\n","                dependencies.add_arc(stack[-2], stack.pop())\n","                return stack,i,dependencies\n","            case 2:\n","                dependencies.add_arc(stack[-1], stack[-2])\n","                stack.pop(-2)\n","                return stack,i,dependencies\n","            case _:\n","                raise \"Wrong Move\"\n","\n","    #returns the possible moves that can be applied in parsing\n","    def get_valid_moves(self,i, n, stack_depth):\n","        moves = []\n","        if i <= n:\n","            moves.append(0)\n","        if stack_depth >= 2:\n","            moves.append(1)\n","            moves.append(2)\n","        return moves\n","    \n","    #given the sentence, the lemma and pos, It returns the depency related to that sentence\n","    def parsing(self,words,phrase_lemma,phrase_pos):\n","        n=len(words)\n","        deps=Dependencies(n)\n","        stack=[0]\n","        i_buffer=1\n","        moves=self.get_valid_moves(i_buffer,n,len(stack))\n","        embeddings = self.oracle.encode(words,phrase_lemma)\n","        while moves:\n","            features_embeddings = self.oracle.extract_embedded_features(embeddings,len(words),[stack],[i_buffer] )\n","            features_pos = self.oracle.extract_pos_features(phrase_pos,[stack],[i_buffer])\n","\n","            features_embeddings=torch.stack(features_embeddings)\n","            features_pos=torch.stack(features_pos)\n","            \n","            scores = self.oracle.score(features_embeddings,features_pos)\n","            scores=scores[-1].tolist()\n","            \n","            next_move = max(moves, key=lambda move: scores[move])\n","            stack,i_buffer,deps = self.transition(next_move, stack, i_buffer, deps)\n","            moves = self.get_valid_moves(i_buffer,n,len(stack))\n","        return deps\n","\n","    #choose the best move for imulate_parse\n","    def check_best(self,heads,stack,buffer,deps,i):\n","        move=-1\n","        if(len(stack)>=2):\n","            children_list=[]\n","            for child,head in enumerate(heads):\n","                if head == stack[-1]:\n","                    children_list.append(child)\n","            if(heads[stack[-2]]==stack[-1]):\n","                move=2\n","            if(((heads[stack[-1]])==(stack[-2])) and all([deps.contains(stack[-1],child) for child in children_list])):\n","                move=1\n","        if(i<=len(buffer) and move==-1):\n","            move=0\n","        elif(i>len(buffer) and move==-1):\n","            move=None\n","        return move\n","    \n","    #it does reverse engineering, given the final state rebuilds the stack, buffer and moves\n","    def simulate_parse(self,heads,buffer):\n","        deps=Dependencies(len(buffer))\n","        stack=[0]\n","        moves=[]\n","        buffers=[]\n","        stacks=[]\n","        i=1\n","        best_move=self.check_best(heads,stack,buffer,deps,i)\n","        while best_move!=None:\n","            buffers.append(i)\n","            stacks.append(stack[:])\n","            moves.append(best_move)\n","            stack,i,deps=self.transition(best_move,stack,i,deps)\n","            best_move=self.check_best(heads,stack,buffer,deps,i)\n","        if(i>len(buffer)):\n","            return stacks,buffers,moves\n","        else: return None\n","\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"elapsed":3768,"status":"error","timestamp":1715785852266,"user":{"displayName":"Daniel Haures","userId":"12706873978284200231"},"user_tz":-120},"id":"9EEaWrtbH_Gh","outputId":"0666cf00-dd1c-4044-eb6c-46dc8e19c2de"},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","0:parsed\n","1\n","1:parsed\n","2\n","2:parsed\n","3\n","3:parsed\n","4\n","4:parsed\n","5\n","5:parsed\n","6\n","6:parsed\n","7\n","7:parsed\n","8\n","8:parsed\n","9\n","9:parsed\n","10\n","10:parsed\n","11\n","11:parsed\n","12\n","12:parsed\n","13\n","13:parsed\n","14\n","14:parsed\n","15\n","15:parsed\n","16\n","16:parsed\n","17\n","17:parsed\n","18\n","18:parsed\n","19\n","19:parsed\n","20\n","20:parsed\n","21\n","21:parsed\n","22\n","22:parsed\n","23\n","23:parsed\n","24\n","24:parsed\n","25\n","25:parsed\n","26\n","26:parsed\n","27\n","27:parsed\n","28\n","28:parsed\n","29\n","29:parsed\n","30\n","30:parsed\n","31\n","31:parsed\n","32\n","32:parsed\n","33\n","33:parsed\n","34\n","34:parsed\n","35\n","35:parsed\n","36\n","36:parsed\n","37\n","37:parsed\n","38\n","38:parsed\n","39\n","39:parsed\n","40\n","40:parsed\n","41\n","41:parsed\n","42\n","42:parsed\n","43\n","43:parsed\n","44\n","44:parsed\n","45\n","45:parsed\n","46\n","46:parsed\n","47\n","47:parsed\n","48\n","48:parsed\n","49\n","49:parsed\n","50\n","50:parsed\n","51\n","51:parsed\n","52\n","52:parsed\n","53\n","53:parsed\n","54\n","54:parsed\n","55\n","55:parsed\n","56\n","56:parsed\n","57\n","57:parsed\n","58\n","58:parsed\n","59\n","59:parsed\n","60\n","60:parsed\n","61\n","61:parsed\n","62\n","62:parsed\n","63\n","63:parsed\n","64\n","64:parsed\n","65\n","65:parsed\n","66\n","66:parsed\n","67\n","67:parsed\n","68\n","68:parsed\n","69\n","69:parsed\n","70\n","70:parsed\n","71\n","71:parsed\n","72\n","72:parsed\n","73\n","73:parsed\n","74\n","74:parsed\n","75\n","75:parsed\n","76\n","76:parsed\n","77\n","77:parsed\n","78\n","78:parsed\n","79\n","79:parsed\n","80\n","80:parsed\n","81\n","81:parsed\n","82\n","82:parsed\n","83\n","83:parsed\n","84\n","84:parsed\n","85\n","85:parsed\n","86\n","86:parsed\n","87\n","87:parsed\n","88\n","88:parsed\n","89\n","89:parsed\n","90\n","90:parsed\n","91\n","91:parsed\n","92\n","92:parsed\n","93\n","93:parsed\n","94\n","94:parsed\n","95\n","95:parsed\n","96\n","96:parsed\n","97\n","97:parsed\n","98\n","98:parsed\n","99\n","99:parsed\n","100\n","100:parsed\n","101\n","101:parsed\n","102\n","102:parsed\n","103\n","103:parsed\n","104\n","104:parsed\n","105\n","105:parsed\n","106\n","106:parsed\n","107\n","107:parsed\n","108\n","108:parsed\n","109\n","109:parsed\n","110\n","110:parsed\n","111\n","111:parsed\n","112\n","112:parsed\n","113\n","113:parsed\n","114\n","114:parsed\n","115\n","115:parsed\n","116\n","116:parsed\n","117\n","117:parsed\n","118\n","118:parsed\n","119\n","119:parsed\n","120\n","120:parsed\n","121\n","121:parsed\n","122\n","122:parsed\n","123\n","123:parsed\n","124\n","124:parsed\n","125\n","125:parsed\n","126\n","126:parsed\n","127\n","127:parsed\n","128\n","128:parsed\n","129\n","129:parsed\n","130\n","130:parsed\n","131\n","131:parsed\n","132\n","132:parsed\n","133\n","133:parsed\n","134\n","134:parsed\n","135\n","135:parsed\n","136\n","136:parsed\n","137\n","137:parsed\n","138\n","138:parsed\n","139\n","139:parsed\n","140\n","140:parsed\n","141\n","141:parsed\n","142\n","142:parsed\n","143\n","143:parsed\n","144\n","144:parsed\n","145\n","145:parsed\n","146\n","146:parsed\n","147\n","147:parsed\n","148\n","148:parsed\n","149\n","149:parsed\n","150\n","150:parsed\n","151\n","151:parsed\n","152\n","152:parsed\n","153\n","153:parsed\n","154\n","154:parsed\n","155\n","155:parsed\n","156\n","156:parsed\n","157\n","157:parsed\n","158\n","158:parsed\n","159\n","159:parsed\n","160\n","160:parsed\n","161\n","161:parsed\n","162\n","162:parsed\n","163\n","163:parsed\n","164\n","164:parsed\n","165\n","165:parsed\n","166\n","166:parsed\n","167\n","167:parsed\n","168\n","168:parsed\n","169\n","169:parsed\n","170\n","170:parsed\n","171\n","171:parsed\n","172\n","172:parsed\n","173\n","173:parsed\n","174\n","174:parsed\n","175\n","175:parsed\n","176\n","176:parsed\n","177\n","177:parsed\n","178\n","178:parsed\n","179\n","179:parsed\n","180\n","180:parsed\n","181\n","181:parsed\n","182\n","182:parsed\n","183\n","183:parsed\n","184\n","184:parsed\n","185\n","185:parsed\n","186\n","186:parsed\n","187\n","187:parsed\n","188\n","188:parsed\n","189\n","189:parsed\n","190\n","190:parsed\n","191\n","191:parsed\n","192\n","192:parsed\n","193\n","193:parsed\n","194\n","194:parsed\n","195\n","195:parsed\n","196\n","196:parsed\n","197\n","197:parsed\n","198\n","198:parsed\n","199\n","199:parsed\n","200\n","200:parsed\n","201\n","201:parsed\n","202\n","202:parsed\n","203\n","203:parsed\n","204\n","204:parsed\n","205\n","205:parsed\n","206\n","206:parsed\n","207\n","207:parsed\n","208\n","208:parsed\n","209\n","209:parsed\n","210\n","210:parsed\n","211\n","211:parsed\n","212\n","212:parsed\n","213\n","213:parsed\n","214\n","214:parsed\n","215\n","215:parsed\n","216\n","216:parsed\n","217\n","217:parsed\n","218\n","218:parsed\n","219\n","219:parsed\n","220\n","220:parsed\n","221\n","221:parsed\n","222\n","222:parsed\n","223\n","223:parsed\n","224\n","224:parsed\n","225\n","225:parsed\n","226\n","226:parsed\n","227\n","227:parsed\n","228\n","228:parsed\n","229\n","229:parsed\n","230\n","230:parsed\n","231\n","231:parsed\n","232\n","232:parsed\n","233\n","233:parsed\n","234\n","234:parsed\n","235\n","235:parsed\n","236\n","236:parsed\n","237\n","237:parsed\n","238\n","238:parsed\n","239\n","239:parsed\n","240\n","240:parsed\n","241\n","241:parsed\n","242\n","242:parsed\n","243\n","243:parsed\n","244\n","244:parsed\n","245\n","245:parsed\n","246\n","246:parsed\n","247\n","247:parsed\n","248\n","248:parsed\n","249\n","249:parsed\n","250\n","250:parsed\n","251\n","251:parsed\n","252\n","252:parsed\n","253\n","253:parsed\n","254\n","254:parsed\n","255\n","255:parsed\n","256\n","256:parsed\n","257\n","257:parsed\n","258\n","258:parsed\n","259\n","259:parsed\n","260\n","260:parsed\n","261\n","261:parsed\n","262\n","262:parsed\n","263\n","263:parsed\n","264\n","264:parsed\n","265\n","265:parsed\n","266\n","266:parsed\n","267\n","267:parsed\n","268\n","268:parsed\n","269\n","269:parsed\n","270\n","270:parsed\n","271\n","271:parsed\n","272\n","272:parsed\n","273\n","273:parsed\n","274\n","274:parsed\n","275\n","275:parsed\n","276\n","276:parsed\n","277\n","277:parsed\n","278\n","278:parsed\n","279\n","279:parsed\n","280\n","280:parsed\n","281\n","281:parsed\n","282\n","282:parsed\n","283\n","283:parsed\n","284\n","284:parsed\n","285\n","285:parsed\n","286\n","286:parsed\n","287\n","287:parsed\n","288\n","288:parsed\n","289\n","289:parsed\n","290\n","290:parsed\n","291\n","291:parsed\n","292\n","292:parsed\n","293\n","293:parsed\n","294\n","294:parsed\n","295\n","295:parsed\n","296\n","296:parsed\n","297\n","297:parsed\n","298\n","298:parsed\n","299\n","299:parsed\n","300\n","300:parsed\n","301\n","301:parsed\n","302\n","302:parsed\n","303\n","303:parsed\n","304\n","304:parsed\n","305\n","305:parsed\n","306\n","306:parsed\n","307\n","307:parsed\n","308\n","308:parsed\n","309\n","309:parsed\n","310\n","310:parsed\n","311\n","311:parsed\n","312\n","312:parsed\n","313\n","313:parsed\n","314\n","314:parsed\n","315\n","315:parsed\n","316\n","316:parsed\n","317\n","317:parsed\n","318\n","318:parsed\n","319\n","319:parsed\n","320\n","320:parsed\n","321\n","321:parsed\n","322\n","322:parsed\n","323\n","323:parsed\n","324\n","324:parsed\n","325\n","325:parsed\n","326\n","326:parsed\n","327\n","327:parsed\n","328\n","328:parsed\n","329\n","329:parsed\n","330\n","330:parsed\n","331\n","331:parsed\n","332\n","332:parsed\n","333\n","333:parsed\n","334\n","334:parsed\n","335\n","335:parsed\n","336\n","336:parsed\n","337\n","337:parsed\n","338\n","338:parsed\n","339\n","339:parsed\n","340\n","340:parsed\n","341\n","341:parsed\n","342\n","342:parsed\n","343\n","343:parsed\n","344\n","344:parsed\n","345\n","345:parsed\n","346\n","346:parsed\n","347\n","347:parsed\n","348\n","348:parsed\n","349\n","349:parsed\n","350\n","350:parsed\n","351\n","351:parsed\n","352\n","352:parsed\n","353\n","353:parsed\n","354\n","354:parsed\n","355\n","355:parsed\n","356\n","356:parsed\n","357\n","357:parsed\n","358\n","358:parsed\n","359\n","359:parsed\n","360\n","360:parsed\n","361\n","361:parsed\n","362\n","362:parsed\n","363\n","363:parsed\n","364\n","364:parsed\n","365\n","365:parsed\n","366\n","366:parsed\n","367\n","367:parsed\n","368\n","368:parsed\n","369\n","369:parsed\n","370\n","370:parsed\n","371\n","371:parsed\n","372\n","372:parsed\n","373\n","373:parsed\n","374\n","374:parsed\n","375\n","375:parsed\n","376\n","376:parsed\n","377\n","377:parsed\n","378\n","378:parsed\n","379\n","379:parsed\n","380\n","380:parsed\n","381\n","381:parsed\n","382\n","382:parsed\n","383\n","383:parsed\n","384\n","384:parsed\n","385\n","385:parsed\n","386\n","386:parsed\n","387\n","387:parsed\n","388\n","388:parsed\n","389\n","389:parsed\n","390\n","390:parsed\n","391\n","391:parsed\n","392\n","392:parsed\n","393\n","393:parsed\n","394\n","394:parsed\n","395\n","395:parsed\n","396\n","396:parsed\n","397\n","397:parsed\n","398\n","398:parsed\n","399\n","399:parsed\n","400\n","400:parsed\n","401\n","401:parsed\n","402\n","402:parsed\n","403\n","403:parsed\n","404\n","404:parsed\n","405\n","405:parsed\n","406\n","406:parsed\n","407\n","407:parsed\n","408\n","408:parsed\n","409\n","409:parsed\n","410\n","410:parsed\n","411\n","411:parsed\n","412\n","412:parsed\n","413\n","413:parsed\n","414\n","414:parsed\n","415\n","415:parsed\n","416\n","416:parsed\n","417\n","417:parsed\n","418\n","418:parsed\n","419\n","419:parsed\n","420\n","420:parsed\n","421\n","421:parsed\n","422\n","422:parsed\n","423\n","423:parsed\n","424\n","424:parsed\n","425\n","425:parsed\n","426\n","426:parsed\n","427\n","427:parsed\n","428\n","428:parsed\n","429\n","429:parsed\n","430\n","430:parsed\n","431\n","431:parsed\n","432\n","432:parsed\n","433\n","433:parsed\n","434\n","434:parsed\n","435\n","435:parsed\n","436\n","436:parsed\n","437\n","437:parsed\n","438\n","438:parsed\n","439\n","439:parsed\n","440\n","440:parsed\n","441\n","441:parsed\n","442\n","442:parsed\n","443\n","443:parsed\n","444\n","444:parsed\n","445\n","445:parsed\n","446\n","446:parsed\n","447\n","447:parsed\n","448\n","448:parsed\n","449\n","449:parsed\n","450\n","450:parsed\n","451\n","° is wrong lemma\n","451:parsed\n","452\n","452:parsed\n","453\n","453:parsed\n","454\n","454:parsed\n","455\n","455:parsed\n","456\n","456:parsed\n","457\n","457:parsed\n","458\n","458:parsed\n","459\n","459:parsed\n","460\n","460:parsed\n","461\n","461:parsed\n","462\n","462:parsed\n","463\n","463:parsed\n","464\n","464:parsed\n","465\n","465:parsed\n","466\n","466:parsed\n","467\n","467:parsed\n","468\n","468:parsed\n","469\n","469:parsed\n","470\n","470:parsed\n","471\n","471:parsed\n","472\n","472:parsed\n","473\n","473:parsed\n","474\n","474:parsed\n","475\n","475:parsed\n","476\n","476:parsed\n","477\n","477:parsed\n","478\n","478:parsed\n","479\n","479:parsed\n","480\n","480:parsed\n","481\n","481:parsed\n","None\n"]}],"source":["#given the phrase and its heads reconstructs the optimal moves to get those heads,also returns the embedding of the stack,buffer and pos\n","def encode_moves(oracle,parser,heads,phrase,phrase_lemma,phrase_pos):\n","    stacks,buffers,moves=parser.simulate_parse(heads,phrase)\n","\n","    embedded_features = oracle.oracle.extract_embedded_features(phrase_pos,stacks,buffers)\n","    pos_features = oracle.oracle.extract_pos_features(phrase_lemma,phrase_lemma,stacks,buffers)\n","\n","    expanded_moves=[]\n","    for move in moves:\n","        if(move==0): expanded_moves.append(torch.tensor([1,0,0]))\n","        if(move==1): expanded_moves.append(torch.tensor([0,1,0]))\n","        if(move==2): expanded_moves.append(torch.tensor([0,0,1]))\n","\n","    return embedded_features,pos_features,expanded_moves\n","\n","#takes the dataset and for each sentence generates the corresponding batch, will generate n=batch-size file\n","def create_batches(batch_size,dataset):\n","\n","    oracle = Oracle()\n","    parser = Parser(oracle)\n","    batch_feature=[]\n","    batch_pos=[]\n","    batch_moves=[]\n","\n","    pos_tags = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN','VERB','ADP','AUX','CCONJ','DET','NUM','PART','PRON','SCONJ','PUNCT','SYM','X']\n","    le = preprocessing.LabelEncoder()\n","    le = le.fit(pos_tags)\n","    \n","    index_batch=0\n","    for sent in dataset:\n","        if(index_batch<200):\n","            heads=[-1]\n","            words=[]\n","            lemmas=[]\n","            pos=[]\n","\n","            wrong_sent=0\n","            for token in sent:\n","                if(token.head is None): wrong_sent=1\n","                if(token.form is None): wrong_sent=1\n","                if(token.lemma is None): wrong_sent=1\n","                if(token.upos is None): wrong_sent=1\n","\n","                heads.append(int(token.head))\n","                words.append(token.form)\n","                lemmas.append(token.lemma)\n","                pos.append(token.upos)\n","\n","            if(wrong_sent==0):\n","\n","                pos_int=le.transform(pos)\n","                sent_features,sent_pos,sent_moves = encode_moves(oracle,parser,heads,words,lemmas,pos_int)\n","                sent_features=torch.stack(sent_features,dim=0).to(torch.float32)\n","                sent_pos=torch.stack(sent_pos).to(torch.float32)\n","                sent_moves=torch.stack(sent_moves).to(torch.float32)\n","\n","                batch_feature.append(sent_features)\n","                batch_pos.append(sent_pos)\n","                batch_moves.append(sent_moves)\n","                \n","\n","\n","            if(len(batch_moves)==batch_size):\n","                print(index_batch)\n","                \n","                packed_features=pack_sequence(batch_feature,enforce_sorted=False)\n","                packed_pos=pack_sequence(batch_pos,enforce_sorted=False)\n","                packed_moves=pack_sequence(batch_moves,enforce_sorted=False)\n","                \n","                torch.save((packed_features,packed_pos,packed_moves), f\"data/batches/tensor{index_batch}.pt\")\n","                index_batch+=1\n","                batch_feature=[]\n","                batch_pos=[]\n","                batch_moves=[]\n","\n","\n","def sentence_to_conllu(tokens):\n","  return '\\n'.join([token.conll() for token in tokens])\n","\n","def predicted_on_test(parser):\n","  #Load e preprocessing\n","  count=0\n","  test = pyconll.load_from_file('data/it_isdt-ud-test.conllu')\n","  test_preprocesed=[]\n","  for i,sent in enumerate(test):\n","      sentence_preprocesed=[]\n","      for j,token in enumerate(sent):\n","          if(token.head is not None):\n","              sentence_preprocesed.append(token)\n","      test_preprocesed.append(sentence_preprocesed)\n","\n","  with open('data/output_real.conllu', 'w', encoding='utf-8') as f:\n","    for sentence in test_preprocesed:\n","        f.write('#sent_id = none\\n')\n","        f.write('#text = none\\n')\n","        f.write(sentence_to_conllu(sentence))\n","        f.write('\\n\\n')\n","\n","\n","\n","  pos_tags = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN','VERB','ADP','AUX','CCONJ','DET','NUM','PART','PRON','SCONJ','PUNCT','SYM','X']\n","  le = preprocessing.LabelEncoder()\n","  le = le.fit(pos_tags)\n","\n","  for i,sent in enumerate(test_preprocesed):\n","    if(i<500):\n","      print(i)\n","      heads=[-1]\n","      words=[]\n","      lemmas=[]\n","      pos=[]\n","      wrong_sent=0\n","      for token in sent:\n","\n","        if(token.head is None):\n","          print(token.form+\" is wrong head\")\n","          wrong_sent=1\n","        else:\n","          heads.append(int(token.head))\n","        if(token.form is None):\n","          print(token.form+\" is wrong form\")\n","          wrong_sent=1\n","        else:\n","          words.append(token.form)\n","        if(token.lemma is None):\n","          print(token.form+\" is wrong lemma\")\n","          lemmas.append(token.form)\n","        else:\n","          lemmas.append(token.lemma)\n","        if(token.upos is None):\n","          print(token.form+\" is wrong upos\")\n","          wrong_sent=1\n","        else:\n","          pos.append(token.upos)\n","\n","      if(wrong_sent!=1):\n","        print(str(i)+\":parsed\")\n","        pos_int=le.transform(pos)\n","        predicted_deps=parser.parsing(words,lemmas,pos_int)\n","        predicted_heads=predicted_deps.get_heads()\n","        for j,atoken in enumerate(sent):\n","           if(atoken.head is None):\n","              atoken.head=str(-1)\n","           else:\n","              atoken.head=str(predicted_heads[j+1])\n","      else:\n","        print(words)\n","        print(str(i)+\":ignored\")\n","        count+=1\n","\n","  #daves the modified file\n","  with open('data/output_predicted.conllu', 'w', encoding='utf-8') as f:\n","    for sentence in test_preprocesed:\n","        f.write('#sent_id = none\\n')\n","        f.write('#text = none\\n')\n","        f.write(sentence_to_conllu(sentence))\n","        f.write('\\n\\n')\n","\n","\n","\n","#create_batches(100,train_prepocesed)\n","oracle = Oracle()\n","oracle.model.load_state_dict(torch.load('data/model_parameters.ml'))\n","#oracle.train_on_batches()\n","parser = Parser(oracle)\n","print(predicted_on_test(parser))\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"p8xFzbYo_UAH"},"source":["hidden 64, epoch 1, lr 0.001, memory 0, bidir False, batches 30 0.7\n","hidden 64, epoch 1, lr 0.01, memory 0, bidir False, batches 30 0.55 overfitting\n","hidden 64, epoch 1, lr 0.0001, memory 0, bidir False, batches 0.58 underfitting\n","\n","hidden 64, epoch 3, lr 0.001, memory 0, bidir False, batches 30 0.74\n","hidden 64, epoch 4, lr 0.001, memory 0, bidir False, batches 30 0.78 top\n","hidden 64, epoch 4, lr 0.001, memory 3, bidir False, batches 30 0.64 !\n","\n","hidden 64, epoch 4, lr 0.001, memory 0, bidir False, batches 60 0.75  overfitting\n","hidden 64, epoch 3, lr 0.001, memory 0, bidir False, batches 60 0.76  overfitting\n","hidden 64, epoch 2, lr 0.001, memory 0, bidir False, batches 60 0.78  uguale a prima\n","hidden 64, epoch 1, lr 0.001, memory 0, bidir False, batches 60 0.71  underfitting\n","\n","hidden 64, epoch 2, lr 0.0005, memory 0, bidir False, batches 120 0.78  \n","hidden 64, epoch 3, lr 0.0005, memory 0, bidir False, batches 120 0.78  \n","hidden 128, epoch 3, lr 0.0005, memory 0, bidir False, batches 120 0.815  top\n","hidden 512, epoch 4, lr 0.0005, memory 0, bidir False, batches 120 0.83  top"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
