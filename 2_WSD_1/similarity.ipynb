{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "from nltk.corpus import wordnet as wn\n",
    "from libs.sym import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Word 1    Word 2  Human (mean)  wu_palmer  shortest_path  leak_chod\n",
      "0            love       sex          6.77   8.460128       9.736842   9.929098\n",
      "1           tiger       cat          7.35   8.854262       9.736842   9.929098\n",
      "2           tiger     tiger         10.00   9.165903      10.000000  10.000000\n",
      "3            book     paper          7.46   8.020165       9.473684   8.037094\n",
      "4        computer  keyboard          7.62   7.552704       9.210526   6.930343\n",
      "..            ...       ...           ...        ...            ...        ...\n",
      "348        shower     flood          6.03   5.829514       8.947368   6.145091\n",
      "349       weather  forecast          8.34   1.219065       6.578947   2.927852\n",
      "350      disaster      area          6.25   4.582951       7.894737   4.253087\n",
      "351      governor    office          6.34   4.821265       7.631579   3.931588\n",
      "352  architecture   century          3.78   2.823098       7.631579   3.931588\n",
      "\n",
      "[353 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>pearson</th>\n",
       "      <th>spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wu &amp; Palmer</td>\n",
       "      <td>0.244442</td>\n",
       "      <td>0.298227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shortest Path</td>\n",
       "      <td>0.061495</td>\n",
       "      <td>0.261141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Leak &amp; Chod</td>\n",
       "      <td>0.272024</td>\n",
       "      <td>0.261141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      similarity   pearson  spearman\n",
       "0    Wu & Palmer  0.244442  0.298227\n",
       "1  Shortest Path  0.061495  0.261141\n",
       "2    Leak & Chod  0.272024  0.261141"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #set-up of dataframe from csv file\n",
    "    word_sym=pd.read_csv('data\\\\WordSim353.csv')\n",
    "    word_sym.insert(3,\"wu_palmer\",np.zeros(353))\n",
    "    word_sym.insert(4,\"shortest_path\",np.zeros(353))\n",
    "    word_sym.insert(5,\"leak_chod\",np.zeros(353))\n",
    "\n",
    "    for i in range(0,353):\n",
    "        w1 = word_sym.at[i,'Word 1']\n",
    "        w2 = word_sym.at[i,'Word 2']\n",
    "        max_wu=0\n",
    "        max_sp=0\n",
    "        max_lch=0\n",
    "        for s1 in wn.synsets(w1):\n",
    "            for s2 in wn.synsets(w2):\n",
    "                #similarity for specific senses of w1 and w2\n",
    "                actual_wu=wu_similarity(s1,s2)\n",
    "                actual_sp=sp_similarity(s1,s2)\n",
    "                actual_lch=lch_similarity(s1,s2)\n",
    "                #senses of w1 and w2 that maximize the similarity value\n",
    "                if actual_wu>max_wu:\n",
    "                    max_wu=actual_wu\n",
    "                if actual_sp>max_sp:\n",
    "                    max_sp=actual_sp\n",
    "                if actual_lch>max_lch:\n",
    "                    max_lch=actual_lch\n",
    "        word_sym.iat[i,3]= max_wu\n",
    "        word_sym.iat[i,4]= max_sp\n",
    "        word_sym.iat[i,5]= max_lch\n",
    "\n",
    "    #min-max normalization of similarity measures\n",
    "    word_sym['wu_palmer']=(word_sym['wu_palmer']-word_sym['wu_palmer'].min())/(word_sym['wu_palmer'].max()-word_sym['wu_palmer'].min())*10\n",
    "    word_sym['shortest_path']=(word_sym['shortest_path']-word_sym['shortest_path'].min())/(word_sym['shortest_path'].max()-word_sym['shortest_path'].min())*10\n",
    "    word_sym['leak_chod']=(word_sym['leak_chod']-word_sym['leak_chod'].min())/(word_sym['leak_chod'].max()-word_sym['leak_chod'].min())*10\n",
    "    \n",
    "    print(word_sym)\n",
    "\n",
    "    #pearson and spearman corellation of similarity measures and human values\n",
    "    results=pd.DataFrame({'similarity':['Wu & Palmer','Shortest Path','Leak & Chod'],\n",
    "                         'pearson':np.zeros(3),\n",
    "                         'spearman':np.zeros(3)})\n",
    "    results.iat[0,1]=scipy.stats.pearsonr(word_sym['Human (mean)'],word_sym['wu_palmer'],alternative='two-sided', method=None)[0]\n",
    "    results.iat[1,1]=scipy.stats.pearsonr(word_sym['Human (mean)'],word_sym['shortest_path'],alternative='two-sided', method=None)[0]\n",
    "    results.iat[2,1]=scipy.stats.pearsonr(word_sym['Human (mean)'],word_sym['leak_chod'],alternative='two-sided', method=None)[0]\n",
    "    results.iat[0,2]=scipy.stats.spearmanr(word_sym['Human (mean)'],word_sym['wu_palmer'], axis=0, nan_policy='propagate', alternative='two-sided')[0]\n",
    "    results.iat[1,2]=scipy.stats.spearmanr(word_sym['Human (mean)'],word_sym['shortest_path'], axis=0, nan_policy='propagate', alternative='two-sided')[0]\n",
    "    results.iat[2,2]=scipy.stats.spearmanr(word_sym['Human (mean)'],word_sym['leak_chod'], axis=0, nan_policy='propagate', alternative='two-sided')[0]\n",
    "\n",
    "    display(results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
