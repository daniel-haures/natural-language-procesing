{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(wn.synset('car.n.01'), wn.synset('car.n.01').definition())\n",
    "print(wn.synset('car.n.01').part_meronyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sense similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "s1 = wn.synset('dog.n.01')\n",
    "s2 = wn.synset('cat.n.01')\n",
    "\n",
    "print('[S1: {}] {}\\n'.format(s1, s1.definition()))\n",
    "print('[S1: {}] {}\\n'.format(s2, s2.definition()))\n",
    "\n",
    "print('wup_similarity({}, {}) = {}'.format(s1, s2, s1.wup_similarity(s2)))\n",
    "print('lch_similarity({}, {}) = {}'.format(s1, s2, s1.lch_similarity(s2)))\n",
    "print('path_similarity({}, {}) = {}'.format(s1, s2, s1.path_similarity(s2)))\n",
    "\n",
    "# NB: wup_similarity Ã¨ la Wu & Palmer similarity, che dobbiamo reimplementare\n",
    "\n",
    "# for ss1 in wn.synsets('dog'):\n",
    "#     for ss2 in wn.synsets('cat'):\n",
    "#         print('sim ' + str(ss1) + ', ' + str(ss2) + ' = ' + str(ss1.wup_similarity(ss2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyponyms(synset):\n",
    "    hyponyms = set()\n",
    "    for hyponym in synset.hyponyms():\n",
    "        hyponyms |= set(get_hyponyms(hyponym))\n",
    "    return hyponyms | set(synset.hyponyms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i synset associati a un termine, e rispettivi iponimi e iperonimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# term = 'board'\n",
    "term = 'bank'\n",
    "print('\\nExploring senses for term \"{}\"\\n'.format(term))\n",
    "\n",
    "for ss in wn.synsets(term):\n",
    "    print('\\n' + str(ss))\n",
    "    print(ss.name(), ss.lemma_names()) \n",
    "    offset = str(ss.offset()).zfill(8) + '-' + ss.pos()\n",
    "    print(f'offset: {offset}\\n')\n",
    "    print('def : ' + ss.definition())\n",
    "    print('ex  : ' + str(ss.examples()))\n",
    "\n",
    "    print('\\n\\t ** Hyponyms **')\n",
    "    for hyp in ss.hyponyms():\n",
    "        print('\\thypon: ' + str(hyp))\n",
    "\n",
    "    print('\\n\\t ** Hypernyms **')\n",
    "    for hyp in ss.hypernyms():\n",
    "        print('\\thyper: ' + str(hyp))\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "syns = list(wn.all_synsets())\n",
    "offsets_list = [(s.offset(), s) for s in syns]\n",
    "\n",
    "print(syns[1:10])\n",
    "print(offsets_list[1:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtro nouns o verbs a partire da offset\n",
    "# offset ha forma: 09213565-n\n",
    "get_n = 'n'\n",
    "get_v = 'v'\n",
    "\n",
    "print('term currently investigated is ' + term)\n",
    "\n",
    "print('--- VERBS ---')\n",
    "for ss in wn.synsets(term):\n",
    "    offset = str(ss.offset()).zfill(8) + '-' + ss.pos()\n",
    "    if offset[-1] == get_v:\n",
    "        print(offset)\n",
    "\n",
    "print('--- NOUNS ---')        \n",
    "for ss in wn.synsets(term):\n",
    "    offset = str(ss.offset()).zfill(8) + '-' + ss.pos()\n",
    "    if offset[-1] == get_n:\n",
    "        print(offset)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iperonimi di un dato synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('pasta.n.01').hyponyms()\n",
    "\n",
    "for hyp in get_hyponyms(wn.synset('pasta.n.01')):\n",
    "    print(hyp)\n",
    "\n",
    "# IPERONIMI\n",
    "term = 'pasta'\n",
    "for ss in wn.synsets(term):\n",
    "    print('\\n\\t ** Hypernyms **')\n",
    "    for hyp in ss.hypernyms():\n",
    "        print('\\thyper: ' + str(hyp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parti e componenti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verbi e entailment (implicazione)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Exploring entailment relation in verbs')\n",
    "verb = 'snore' # RUSSARE...\n",
    "count = 1\n",
    "# NB: con il secondo parametro selezioniamo il PoS, in questo caso VB\n",
    "for ss in wn.synsets(verb, 'v'):\n",
    "    print('sense {}({}): to {} implies to {}'.format(count, ss, verb, ss.entailments()))\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### esploro come funzionano i nomi composti (pick up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'add_on'\n",
    "# term = 'give_up'\n",
    "print('\\nExploring senses for term \"{}\"\\n'.format(term))\n",
    "\n",
    "for ss in wn.synsets(term):\n",
    "    print('\\n' + str(ss))\n",
    "    print(ss.name(), ss.lemma_names()) \n",
    "    print('def : ' + ss.definition())\n",
    "    print('ex  : ' + str(ss.examples()))\n",
    "\n",
    "#     print('\\n\\t ** Hyponyms **')\n",
    "#     for hyp in ss.hyponyms():\n",
    "#         print('\\thypon: ' + str(hyp))\n",
    "\n",
    "#     print('\\n\\t ** Hypernyms **')\n",
    "#     for hyp in ss.hypernyms():\n",
    "#         print('\\thyper: ' + str(hyp))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libreria wordnet in nltk\n",
    "documentazione disponibile all'URL\n",
    "[https://www.nltk.org/howto/wordnet.html](https://www.nltk.org/howto/wordnet.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sense similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i synset associati a un termine, e rispettivi iponimi e iperonimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "syns = list(wn.all_synsets())\n",
    "offsets_list = [(s.offset(), s) for s in syns]\n",
    "\n",
    "print(syns[1:10])\n",
    "print(offsets_list[1:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iperonimi di un dato synset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parti e componenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Exploring entailment relation in verbs')\n",
    "verb = 'snore' # RUSSARE...\n",
    "count = 1\n",
    "# NB: con il secondo parametro selezioniamo il PoS, in questo caso VB\n",
    "for ss in wn.synsets(verb, 'v'):\n",
    "    print('sense {}({}): to {} implies to {}'.format(count, ss, verb, ss.entailments()))\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'add_on'\n",
    "# term = 'give_up'\n",
    "print('\\nExploring senses for term \"{}\"\\n'.format(term))\n",
    "\n",
    "for ss in wn.synsets(term):\n",
    "    print('\\n' + str(ss))\n",
    "    print(ss.name(), ss.lemma_names()) \n",
    "    print('def : ' + ss.definition())\n",
    "    print('ex  : ' + str(ss.examples()))\n",
    "\n",
    "#     print('\\n\\t ** Hyponyms **')\n",
    "#     for hyp in ss.hyponyms():\n",
    "#         print('\\thypon: ' + str(hyp))\n",
    "\n",
    "#     print('\\n\\t ** Hypernyms **')\n",
    "#     for hyp in ss.hypernyms():\n",
    "#         print('\\thyper: ' + str(hyp))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
