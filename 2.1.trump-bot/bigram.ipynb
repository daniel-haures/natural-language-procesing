{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "\n",
    "LEN_VOCABULARY=1866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_repeated_key ['it’s all']\n",
      "entro in rp\n",
      "no_repeated_key ['all haters']\n",
      "entro in rp\n",
      "no_repeated_key ['haters and']\n",
      "entro in rp\n",
      "no_repeated_key ['and losers!']\n",
      "entro in rp\n",
      "no_repeated_key ['losers! donaldtrump']\n",
      "entro in rp\n",
      "no_repeated_key ['donaldtrump is']\n",
      "entro in rp\n",
      "no_repeated_key ['is a']\n",
      "entro in rp\n",
      "no_repeated_key ['a loser']\n",
      "entro in rp\n",
      "no_repeated_key ['loser until']\n",
      "entro in rp\n",
      "no_repeated_key ['until you']\n",
      "entro in rp\n",
      "no_repeated_key ['you are']\n",
      "entro in rp\n",
      "no_repeated_key ['are losers']\n",
      "entro in rp\n",
      "no_repeated_key ['losers and']\n",
      "entro in rp\n",
      "no_repeated_key []\n",
      "chosen key entrato ['and', 'losers']\n",
      "no_repeated_key ['losers <end>']\n",
      "entro in rp\n",
      "Trump posted on 2-6-2024 at 12:23 : it’s all haters and losers! donaldtrump is a loser until you are losers and losers \n",
      "Perplexity of the tweet: 10.643995970117059\n"
     ]
    }
   ],
   "source": [
    "tweet_df=pd.read_csv('data\\\\tweets.csv')\n",
    "\n",
    "#ignores the retweets\n",
    "tweet_df=tweet_df[tweet_df['is_retweet'] == False]\n",
    "tweet_df=tweet_df[\"text\"].astype(str).values.tolist()\n",
    "\n",
    "def clean_tweets(tweet_list):\n",
    "    cleaned_tweets = []\n",
    "    char_rem = list(set(\"#$%'()*+,./:;<=>?[\\]^_`{|}~\" + \"“\" + \"”\"))\n",
    "    for tweet in tweet_list:\n",
    "        # Removes links starting with \"https://\"\n",
    "        cleaned_tweet = re.sub(r'https?://\\S+', '', tweet)\n",
    "        cleaned_tweet = re.sub('\\s-\\s',' ', cleaned_tweet)\n",
    "        # Makes all characters lowercase\n",
    "        cleaned_tweet = cleaned_tweet.lower()\n",
    "        # ignores characters that are inside char_rem\n",
    "        cleaned_tweet = ''.join(char for char in cleaned_tweet if char not in char_rem)\n",
    "        cleaned_tweets.append(cleaned_tweet)\n",
    "    array_tweets = [tweet.split() for tweet in cleaned_tweets]\n",
    "    return array_tweets\n",
    "\n",
    "#add start tag and end tag\n",
    "def add_begin_end(tweet_list):\n",
    "    for tweet in tweet_list:\n",
    "        tweet.insert(0,\"<b>\")\n",
    "        tweet.append(\"<end>\")\n",
    "    return tweet_list\n",
    "\n",
    "#generates all the bigrams from array_tweets, and saves them in the bidict dictionary key:bigram value:occurence\n",
    "def bigrams_generator(array_tweets):\n",
    "    bidict = {}\n",
    "    for tweet in array_tweets:\n",
    "        for i in range(0,len(tweet)-1):\n",
    "            jword= tweet[i]+\" \"+tweet[i+1]\n",
    "            if jword in bidict:\n",
    "                bidict.update({jword:(bidict.get(jword)+1)})\n",
    "            else:\n",
    "                bidict.update({jword:1})\n",
    "    return bidict            \n",
    "\n",
    "#calculates the occurence of every single word (unigram) key:unigram value:occurence\n",
    "def unigram_generator(array_tweets):\n",
    "    wdict = {}\n",
    "    for tweet in array_tweets:\n",
    "        for word in tweet:\n",
    "            if word in wdict:\n",
    "                wdict.update({word:(wdict.get(word)+1)})\n",
    "            else:\n",
    "                wdict.update({word:1})\n",
    "    return wdict\n",
    "\n",
    "#generates a tweet from a random bigram\n",
    "def generate_tweet(bigram_dictionary):\n",
    "    starting_point=[key for key in bigram_dictionary if key.startswith(\"<b>\")]\n",
    "    sentence=random.choice(starting_point).split()\n",
    "    while(sentence[-1]!=\"<end>\"):\n",
    "        partial_key=sentence[-1]\n",
    "        possible_tail=[possible_key for possible_key in bigram_dictionary if possible_key.split()[0]==partial_key]\n",
    "        possible_tail=sorted(possible_tail,key= lambda x: bigram_dict.get(x),reverse=True)[:6]\n",
    "        #print(possible_tail)\n",
    "        possible_probability= list(map(lambda x: bigram_dict.get(x), possible_tail))\n",
    "        #print(possible_probability)\n",
    "        chosen_key=random.choices(possible_tail,weights=possible_probability, k=3)\n",
    "        #print(\"chosen key \"+str(chosen_key))\n",
    "        no_repeated_key = list(filter(lambda key: key not in \" \".join(sentence), chosen_key))\n",
    "        print(\"no_repeated_key \"+str(no_repeated_key))\n",
    "        if no_repeated_key:\n",
    "            #print(no_repeated_key[0].split())\n",
    "            print(\"entro in rp\")\n",
    "            ck=no_repeated_key[0].split()\n",
    "            sentence.append(ck[-1])\n",
    "        else:\n",
    "            print(\"chosen key entrato \"+str(chosen_key[0].split()))\n",
    "            ck=chosen_key[0].split()\n",
    "            sentence.append(ck[-1])\n",
    "\n",
    "    return sentence\n",
    "\n",
    "#probrability estimation of each trigram with normalization\n",
    "def estimate_probability(bigram_dict,unigram_dict):\n",
    "    for key in bigram_dict:\n",
    "        first_word=key.split()[0]\n",
    "        freq_fw=unigram_dict.get(first_word)\n",
    "        bigram_dict.update({key:((bigram_dict.get(key)+1)/(freq_fw+LEN_VOCABULARY))})\n",
    "    return bigram_dict\n",
    "\n",
    "# adds \"Trump posted on\" before the generated tweet\n",
    "def ultimate_tweet(generated_tweet):\n",
    "    str_tweet = ' '.join(map(str,generated_tweet))\n",
    "    current_date = datetime.datetime.now()\n",
    "    current_hour = time.strftime(\"%H:%M\")\n",
    "    str_tweet = str_tweet.replace(\"<b>\", \"Trump posted on \" + str(current_date.day) + \"-\" + str(current_date.month) + \"-\" + str(current_date.year) + \" at \" + str(current_hour) + \" :\")\n",
    "    str_tweet = str_tweet.replace(\"<end>\", \"\")\n",
    "    return str_tweet\n",
    "\n",
    "#calcultes the perplexity of the genrewted tweet \n",
    "def perplexity(sentence,bigram_dictionary):\n",
    "    result = 0\n",
    "    for i in range(0,len(sentence)-1):\n",
    "        bigram = sentence[i]+\" \"+sentence[i+1]\n",
    "        bprobability = bigram_dictionary.get(bigram)\n",
    "        result=result+math.log10(bprobability)\n",
    "    result = math.exp(-(result/(len(sentence)-1)))\n",
    "    return result\n",
    "\n",
    "\n",
    "array_tweets = clean_tweets(tweet_df)\n",
    "array_tweets= add_begin_end(array_tweets) \n",
    "bigram_dict = bigrams_generator(array_tweets)\n",
    "unigram_dict= unigram_generator(array_tweets)\n",
    "\n",
    "bigram_dict = estimate_probability(bigram_dict,unigram_dict)\n",
    "#print(bigram_dict)\n",
    "\n",
    "generated_tweet = generate_tweet(bigram_dict)\n",
    "\n",
    "print(ultimate_tweet(generated_tweet))\n",
    "\n",
    "print(\"Perplexity of the tweet: \"+str(perplexity(generated_tweet,bigram_dict)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
