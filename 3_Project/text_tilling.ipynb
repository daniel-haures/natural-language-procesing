{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-mpnet-base-v2')  \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzioni di utilità\n",
    "\n",
    "Funzioni neccessarie al trattamento dei documenti e della frasi, ossia pulizia, suddivizione in blochi e ricerca degli indici di inizio sezione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_phrase(phrase):\n",
    "\t\"\"\"clean_phrase funzione che rimuove le stopwords, segni di punteggiatura e numeri\n",
    "\n",
    "\t:param phrase: frase da pulire\n",
    "\t:return: frase pulita\n",
    "\t\"\"\"\n",
    "\twords = word_tokenize(phrase.lower())\n",
    "\tcleaned_words = [word for word in words if word.lower() not in stop_words and word not in punctuation and not word.isdigit() and word !=\"''\"]\n",
    "\treturn cleaned_words\n",
    "\n",
    "def extrack_blocks(N_BLOCK, doc):\n",
    "\t\"\"\"\n",
    "\textrack_blocks: funzione che crea i blocchi, ciascuno dei quali conterrà N_BLOCK frasi\n",
    "\t:N_BLOCK: tagli attesi nel documento\n",
    "\t:doc: documento dal quale creare i blocchi\n",
    "\t:return: lista contenente i blocchi appena creati\n",
    "\t\"\"\"\n",
    "  \n",
    "\tblocks = []\n",
    "\tfor i in range(0, len(doc), N_BLOCK):\n",
    "\t\tif i + (N_BLOCK-1) < len(doc):\n",
    "\t\t\tblocks.append(doc[i:i + N_BLOCK])\n",
    "\t\telse:\n",
    "\t\t\tblocks.append(doc[i:])\n",
    "\treturn blocks\n",
    "\n",
    "def find_section_start_indices(text):\n",
    "\t\"\"\"find_section_start_indices funzione che trova tutte le occorrenze di '<SECTION_SEP>' in un documento\n",
    " \n",
    "\t:text: testo del documento da analizzare\n",
    "\t:return: gli indici che indicano i tagli corretti all'interno di un documento\n",
    "\t\"\"\"  \n",
    "\tsections=text.split('<SECTION_SEP>')\n",
    "\tboundaries=[]\n",
    "\tcount=1\n",
    "\tfor sec in sections[:-1]:\n",
    "\t\tsentences=sec.split('<SENTENCE_SEP>')[:-1]\n",
    "\t\tcount+=len(sentences)\n",
    "\t\tboundaries.append(count)\n",
    "\treturn boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzioni di similarità\n",
    "Le funzioni elencate sotto rappresentano due possibili approci per la missura della similarità tra due 'blocchi' di frasi. \n",
    "Il primo approccio lavora su token non lessicalizzati e su un approccio alla semantica di tipo statisco, la similarità tra due blocchi di frasi viene missurata dall'occorenza simultanea di un termine in entrambi i blocchi. \n",
    "\n",
    "Il secondo approccio è di tipo semantico distribuzionale, per ogni frase di ogni blocco calcola gli embedding e successivamente restituisce la similarità massima raggiunta dal prodotto scalare dei due blocchi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lexical_similarity_fun(block1,block2):\n",
    "\t\"\"\"lexical_similarity_fun funzione che calcola la cosine similarity tra due blocchi\n",
    "\n",
    "\t:block1: primo blocco di frasi da confrontare\n",
    "\t:block2: secondo blocco di frasi da confrontare\n",
    "\t:return: valore della cosine similarity tra due blocchi\n",
    "\t\"\"\"\n",
    "\tblock1 = [clean_phrase(phrase) for phrase in block1]\n",
    "\tblock2 = [clean_phrase(phrase) for phrase in block2]\n",
    "\tblock1_merged= [word for sublist in block1 for word in sublist]\n",
    "\tblock2_merged= [word for sublist in block2 for word in sublist]\n",
    "\tblock1_dict = Counter(block1_merged)\n",
    "\tblock2_dict = Counter(block2_merged)\n",
    "\twords = list(block1_dict.keys() | block2_dict.keys())\n",
    "\tnumerator=0\n",
    "\tdenominator_1=0\n",
    "\tdenominator_2=0\n",
    "\tfor word in words:\n",
    "\t\tnumerator+=block1_dict[word]*block2_dict[word]\n",
    "\t\tdenominator_1 += block1_dict[word]**2  \n",
    "\t\tdenominator_2 += block2_dict[word]**2\n",
    "\tcos_sim=numerator / math.sqrt(denominator_1 * denominator_2)\n",
    "\treturn cos_sim\n",
    "\n",
    "def semantic_similarity_fun(block1, block2):\n",
    "\t\"\"\" semantic_similarity_fun funzione che effettua gli embedding dei due blocchi ne calcola la similarità\n",
    "\n",
    "\t:block1: primo blocco di frasi da confrontare\n",
    "\t:block2: secondo blocco di frasi da confrontare\n",
    "\t:return: valore medio del dot product tra i due blocchi (dot average)\n",
    "\t\"\"\"\n",
    "\n",
    "\tembeddings1 = model.encode(block1)\n",
    "\tembeddings2 = model.encode(block2)\n",
    "\tsimilarities = np.dot(embeddings1, np.transpose(embeddings2))\n",
    "\tsimilarity = np.max(similarities)\n",
    "\t\n",
    "\treturn similarity   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzione di visualizzazione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_similiraty(graph,similarity,smoothed_similarities,actual_boundaries,predicted_boundaries,block_len,name_tag):\n",
    "\t\"\"\"\n",
    "\tplot_similiraty funzione che mostra i risultati ottenuti sotto forma di grafo\n",
    "\t:graph: \n",
    "\t:similarity: valori di similarità ottenuto\n",
    "\t:smoothed_similarities: valori di similarità ottenuto smoothed\n",
    "\t:actual_boundaries: tagli attesi nel documento\n",
    "\t:predicted_boundaries: tagli calcolati tramite la lexical o semantic similarity\n",
    "\t:block_len: numero di frasi contenute in un blocco\n",
    "\t:name_tag: titolo del grafico\n",
    "\t:return: -\n",
    "\t\"\"\" \n",
    "\txpoints = np.arange(1,(len(smoothed_similarities)*block_len)+1,block_len)\n",
    "\typoints = similarity\n",
    "\txpoints_smooth = np.arange(1,(len(smoothed_similarities)*block_len)+1,block_len)\n",
    "\typoints_smooth = smoothed_similarities\n",
    "\tgraph.plot(xpoints, ypoints, '-o', color='red', label='Similarities')\n",
    "\tgraph.plot(xpoints_smooth, ypoints_smooth, '-o', color='blue', label='Smoothed similarities')\n",
    "\tgraph.set_title(name_tag) \n",
    "\n",
    "\t\t\t\t\t\n",
    "\tfor a_b in actual_boundaries:\n",
    "\t\tgraph.axvline(x=(a_b-1), color='green', label='Actual boundaries')\n",
    "\n",
    "\t\n",
    "\tfor p_d in predicted_boundaries:\n",
    "\t\tif p_d not in actual_boundaries:\n",
    "\t\t\tgraph.axvline(x=(p_d-1), color='magenta', label='Predicted boundaries')\n",
    "\t\telse:\n",
    "\t\t\tgraph.axvline(x=(p_d-1), color='black', label='Matched boundaries')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzioni per il calcolo e la valutazione dei boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_boundaries(similarities,block_len,SECTION_SENS):\n",
    "\t\"\"\"\n",
    "\tget_boundaries funzione che, dati gli score di similarità, trova i tagli secondo la formula  (y_a_{i-1} - y_a_{i}) + (y_a_{i+1} - y_a_{i})\n",
    "\t:similarities: vettore contenete gli score di similarità relativi ad un documento\n",
    "\t:block_len: numero di frasi contenute in un blocco\n",
    "\t:SECTION_SENS: valore che regola come un depthscore venga calcolato come boundary (mean-std/SECTION_SENS)\n",
    "\t:return: una lista contenente tutti i tagli effettuati su un dato documento\n",
    "\t\"\"\"  \n",
    "\tdepth_score_sim=[]\n",
    "\tfor i in range(1,len(similarities)-1):\n",
    "\t\tscore=(similarities[i-1]-similarities[i])+(similarities[i+1]-similarities[i])\n",
    "\t\tdepth_score_sim.append(score)\n",
    "\tdepth_score_sim.insert(0,similarities[1]-similarities[0])\n",
    "\tdepth_score_sim.append(similarities[-2]-similarities[-1])\n",
    "\n",
    "\tfiltered=list(filter(lambda x: x > 0, depth_score_sim))\n",
    "\tstd=np.std(filtered)\n",
    "\tmean=np.mean(filtered)\n",
    "\tthreshold=(mean-(std/SECTION_SENS))\n",
    "\n",
    "\tboundaries= [i for i, v in enumerate(depth_score_sim) if v > threshold]\n",
    "\tboundaries = [(x*block_len)+2 for x in boundaries]\n",
    "\treturn boundaries\n",
    "\n",
    "def boundaries_error(actual,predicted):\n",
    "\t\"\"\"\n",
    "\tboundaries_error: funzione che calcola il numero di tagli applicati male all'interno di un documento\n",
    "\t:actual: tagli attesi nel documento\n",
    "\t:predicted: tagli calcolati tramite la cosine similarities o il dot product\n",
    "\t:return: il numero di tagli sbagliati in un documento\n",
    "\t\"\"\"\n",
    "\terrors=[]\n",
    "\tfor boundary in actual:\n",
    "\t\tmin_edit_distance=6000\n",
    "\t\tfor pr in predicted:\n",
    "\t\t\tedit_distance=abs(boundary-pr)\n",
    "\t\t\tif(edit_distance<min_edit_distance):\n",
    "\t\t\t\tmin_edit_distance=edit_distance\n",
    "\t\terrors.append(min_edit_distance)\n",
    "\treturn sum(errors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento del dataset di documenti\n",
    "\n",
    "I singoli documenti sono stati memorizzati all'interno di un file csv, ogni documento è composto da una stringa in cui il token <b>SENTENCE_SEP</b> separa una frase dall'altra e il token <b>SECTION_SEP</b> separa le sezioni del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#contiene i documenti\n",
    "df = pd.read_csv(\"data\\\\wiki_definitions.csv\")  \n",
    "docs=df[\"definitions\"].astype(str).tolist()\n",
    "doc_names=df[\"names\"].astype(str).tolist()\n",
    "\n",
    "\n",
    "#contiene, per ogni documento, gli indici dei separatori di paragrafo (in base alla frase)\n",
    "docs_sections=[]\n",
    "for doc in docs:\n",
    "    sections_pos=find_section_start_indices(doc)\n",
    "    docs_sections.append(sections_pos)\n",
    "\n",
    "\n",
    "#contiene i documenti, ma separati frase per frase\n",
    "docs_sent_by_sent=[]\n",
    "for doc in docs:\n",
    "    doc_t=doc.replace('<SECTION_SEP>','')\n",
    "    doc_t=doc_t.replace('\\r', '').replace('\\n', '')\n",
    "    doc_splitted=doc_t.split('<SENTENCE_SEP>')\n",
    "    docs_sent_by_sent.append(doc_splitted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementazione\n",
    "\n",
    "Escludendo l'impostazione degli iper-parametri, il codice agisce sui singoli documenti nella seguente maniera:\n",
    "\n",
    "1. Suddivisione delle frasi del documento in blocchi di lunghezza <b>N_BLOCK</b>\n",
    "2. Calcolo delle misure di similarità inter-blocco tramite i due approci precedentemente descritti\n",
    "3. Smoothing delle misure di similarità \n",
    "4. Estrazzione degli indici di taglio di sezione nelle misure di similarità\n",
    "\n",
    "Al fine di non iterare nuovamente sui documenti sono stati calcolati per ogni documento i grafici e l'errore, tali dati saranno successivamente visualizzati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x14aab7c2e00>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Numero di frasi contenute in ogni blocco #1 0.6 3 per multi qa\n",
    "N_BLOCK=1\n",
    "#Valore che regola il livello di smoothing del similarity score\n",
    "SMOOTH=0.7\n",
    "#Valore che regola se un depthscore venga considerato come boundary o meno (mean-std/SECTION_SENS)\n",
    "SECTION_SENS=2\n",
    "\n",
    "#Genera (len(docs))*2 sotto grafici\n",
    "figure, axis = plt.subplots(len(docs), 2) \n",
    "figure.set_size_inches(18.5, 8*(len(docs)))\n",
    "\n",
    "boundaries_list=[]\n",
    "boundaries_error_list=[]\n",
    "\n",
    "for DOC_NUM in range(0,len(docs)):\n",
    "\n",
    "    doc=docs_sent_by_sent[DOC_NUM]\n",
    "    blocks = extrack_blocks(N_BLOCK, doc)\n",
    "\n",
    "    #Calcola la similarità per ciascun blocco\n",
    "    lexical_similarities=[]\n",
    "    transformer_similarities=[]\n",
    "    for i in range(0,len(blocks)-1):\n",
    "\n",
    "        lex_similarity=lexical_similarity_fun(blocks[i],blocks[i+1])\n",
    "        lexical_similarities.append(lex_similarity)\n",
    "        bert_similarity=semantic_similarity_fun(blocks[i],blocks[i+1])\n",
    "        transformer_similarities.append(bert_similarity)\n",
    "\n",
    "    #Applica il filtro gaussiano agli score di similarità\n",
    "    smoothed_lexical_similarities=gaussian_filter1d(lexical_similarities, sigma=SMOOTH)\n",
    "    smoothed_transformer_similarities=gaussian_filter1d(transformer_similarities, sigma=SMOOTH)\n",
    "\n",
    "    #Estrai i tagli e calcola l'errore per la similarità lessicale \n",
    "    boundaries_lex=get_boundaries(smoothed_lexical_similarities,N_BLOCK,SECTION_SENS)\n",
    "    boundaries_error_lex=boundaries_error(docs_sections[DOC_NUM],boundaries_lex)\n",
    "\n",
    "    #Estrai i tagli e calcola l'errore per la similarità semantica\n",
    "    boundaries_emb=get_boundaries(smoothed_transformer_similarities,N_BLOCK,SECTION_SENS)\n",
    "    boundaries_error_emb=boundaries_error(docs_sections[DOC_NUM],boundaries_emb)\n",
    "\n",
    "    #Salva i risultati come tuple (lexical output, semantic output)\n",
    "    boundaries_list.append((boundaries_lex,boundaries_emb))\n",
    "    boundaries_error_list.append((boundaries_error_lex,boundaries_error_emb))\n",
    "\n",
    "    #Visualizza i risultati in due grafi per ciascun approccio\n",
    "    plot_similiraty(axis[DOC_NUM][0],lexical_similarities,smoothed_lexical_similarities,docs_sections[DOC_NUM],boundaries_lex,N_BLOCK,name_tag=\"Lexical similary for \"+doc_names[DOC_NUM])\n",
    "    plot_similiraty(axis[DOC_NUM][1],transformer_similarities,smoothed_transformer_similarities,docs_sections[DOC_NUM],boundaries_emb,N_BLOCK,name_tag=\"Semantic similary for \"+doc_names[DOC_NUM])\n",
    "\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcolo e stima dell'errore \n",
    "\n",
    "L'errore è stato quantificato in due modi diversi:\n",
    "1. Distanza dalle sezioni reali rispetto alla più prossima sezione predetta\n",
    "2. Differenza tra il numero di sezioni reali e il numero di sezioni predette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>lexical_match_error</th>\n",
       "      <th>semantic_match_error</th>\n",
       "      <th>lexical_number_of_section_error</th>\n",
       "      <th>semantic_number_of_section_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paxton</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paper</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>punk</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer science</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emotions</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TOTAL</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           document  lexical_match_error  semantic_match_error  \\\n",
       "0             apple                    2                     3   \n",
       "1            paxton                    2                     3   \n",
       "2             paper                    2                     1   \n",
       "3              punk                    3                     5   \n",
       "4  computer science                    8                     3   \n",
       "5          emotions                    4                     0   \n",
       "6             TOTAL                   21                    15   \n",
       "\n",
       "   lexical_number_of_section_error  semantic_number_of_section_error  \n",
       "0                                2                                 1  \n",
       "1                               13                                 7  \n",
       "2                                2                                 3  \n",
       "3                                2                                 0  \n",
       "4                                4                                 0  \n",
       "5                                2                                 1  \n",
       "6                               25                                12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical (match+section) error 46\n",
      "Semantinc (match+section) error 27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>lexical_predicted_boundaries</th>\n",
       "      <th>semantic_predicted_boundaries</th>\n",
       "      <th>real_boundaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>[5, 8, 10, 15, 16]</td>\n",
       "      <td>[4, 10, 12, 16]</td>\n",
       "      <td>[4, 7, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paxton</td>\n",
       "      <td>[3, 4, 6, 8, 9, 11, 14, 18, 19, 21, 26, 29, 30...</td>\n",
       "      <td>[3, 6, 8, 9, 14, 19, 21, 26, 29, 31]</td>\n",
       "      <td>[4, 23, 26]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paper</td>\n",
       "      <td>[8, 10, 14, 18, 22, 23]</td>\n",
       "      <td>[3, 5, 7, 13, 14, 18, 22]</td>\n",
       "      <td>[7, 15, 18, 22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>punk</td>\n",
       "      <td>[4, 7, 8, 11, 15, 19, 23]</td>\n",
       "      <td>[5, 11, 13, 19, 22]</td>\n",
       "      <td>[3, 8, 11, 13, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer science</td>\n",
       "      <td>[8, 12, 13, 18, 19, 21, 22, 24, 27, 28, 31]</td>\n",
       "      <td>[7, 11, 15, 19, 22, 24, 28]</td>\n",
       "      <td>[4, 7, 11, 15, 19, 24, 28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emotions</td>\n",
       "      <td>[3, 4, 10, 12, 15, 16]</td>\n",
       "      <td>[3, 5, 7, 11, 15]</td>\n",
       "      <td>[3, 7, 11, 15]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           document                       lexical_predicted_boundaries  \\\n",
       "0             apple                                 [5, 8, 10, 15, 16]   \n",
       "1            paxton  [3, 4, 6, 8, 9, 11, 14, 18, 19, 21, 26, 29, 30...   \n",
       "2             paper                            [8, 10, 14, 18, 22, 23]   \n",
       "3              punk                          [4, 7, 8, 11, 15, 19, 23]   \n",
       "4  computer science        [8, 12, 13, 18, 19, 21, 22, 24, 27, 28, 31]   \n",
       "5          emotions                             [3, 4, 10, 12, 15, 16]   \n",
       "\n",
       "          semantic_predicted_boundaries             real_boundaries  \n",
       "0                       [4, 10, 12, 16]                  [4, 7, 10]  \n",
       "1  [3, 6, 8, 9, 14, 19, 21, 26, 29, 31]                 [4, 23, 26]  \n",
       "2             [3, 5, 7, 13, 14, 18, 22]             [7, 15, 18, 22]  \n",
       "3                   [5, 11, 13, 19, 22]          [3, 8, 11, 13, 19]  \n",
       "4           [7, 11, 15, 19, 22, 24, 28]  [4, 7, 11, 15, 19, 24, 28]  \n",
       "5                     [3, 5, 7, 11, 15]              [3, 7, 11, 15]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = {'document': ['apple','paxton', 'paper', 'punk','computer science','emotions'], \n",
    "          'lexical_match_error': [i[0] for i in boundaries_error_list],\n",
    "          'semantic_match_error':  [i[1] for i in boundaries_error_list],\n",
    "          'lexical_number_of_section_error':  [abs(len(boundaries_list[i][0])-len(docs_sections[i])) for i in range(0,len(docs))] ,\n",
    "          'semantic_number_of_section_error':  [abs(len(boundaries_list[i][1])-len(docs_sections[i])) for i in range(0,len(docs))] \n",
    "          }\n",
    "\n",
    "pd_errors=pd.DataFrame(errors)\n",
    "total= pd_errors.sum(numeric_only=True, axis=0)\n",
    "pd_errors.loc[len(pd_errors.index)] = ['TOTAL', total['lexical_match_error'], total['semantic_match_error'],total['lexical_number_of_section_error'],total['semantic_number_of_section_error']] \n",
    "\n",
    "display(pd_errors)\n",
    "\n",
    "print(\"Lexical (match+section) error \" + str(total['lexical_match_error']+total['lexical_number_of_section_error']))\n",
    "print(\"Semantinc (match+section) error \" + str(total['semantic_match_error']+total['semantic_number_of_section_error']))\n",
    "\n",
    "\n",
    "boundaries_to_pd = {'document': ['apple','paxton', 'paper', 'punk','computer science','emotions'], 'lexical_predicted_boundaries': [i[0] for i in boundaries_list],'semantic_predicted_boundaries':  [i[1] for i in boundaries_list],'real_boundaries':  docs_sections}\n",
    "display(pd.DataFrame(boundaries_to_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot dei risualtati ottenuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "legend_handles, labels = axis[4][1].get_legend_handles_labels()\n",
    "label_to_handle = dict(zip(labels, legend_handles))\n",
    "unique_labels = set(labels)\n",
    "filtered_handles = []\n",
    "for label in unique_labels:\n",
    "    if label in label_to_handle:\n",
    "        filtered_handles.append(label_to_handle[label])\n",
    "figure.legend(filtered_handles, list(unique_labels),bbox_to_anchor=(0.55,0.92))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
