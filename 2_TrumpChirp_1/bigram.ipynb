{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "LEN_VOCABULARY=1866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df=pd.read_csv('data\\\\tweets.csv')\n",
    "\n",
    "#ignores the retweets\n",
    "tweet_df=tweet_df[tweet_df['is_retweet'] == False]\n",
    "tweet_df=tweet_df[\"text\"].astype(str).values.tolist()\n",
    "\n",
    "def clean_tweets(tweet_list):\n",
    "    cleaned_tweets = []\n",
    "    char_rem = list(set(\"#$%'()*+,./:;<=>?[\\]^_`{|}~\" + \"“\" + \"”\"))\n",
    "    for tweet in tweet_list:\n",
    "        # Removes links starting with \"https://\"\n",
    "        cleaned_tweet = re.sub(r'https?://\\S+', '', tweet)\n",
    "        cleaned_tweet = re.sub('\\s-\\s',' ', cleaned_tweet)\n",
    "        # Makes all characters lowercase\n",
    "        cleaned_tweet = cleaned_tweet.lower()\n",
    "        # ignores characters that are inside char_rem\n",
    "        cleaned_tweet = ''.join(char for char in cleaned_tweet if char not in char_rem)\n",
    "        cleaned_tweets.append(cleaned_tweet)\n",
    "    array_tweets = [tweet.split() for tweet in cleaned_tweets]\n",
    "    return array_tweets\n",
    "\n",
    "#add start tag and end tag\n",
    "def add_begin_end(tweet_list):\n",
    "    for tweet in tweet_list:\n",
    "        tweet.insert(0,\"<b>\")\n",
    "        tweet.append(\"<end>\")\n",
    "    return tweet_list\n",
    "\n",
    "#generates all the bigrams from array_tweets, and saves them in the bidict dictionary key:bigram value:occurence\n",
    "def bigrams_generator(array_tweets):\n",
    "    bidict = {}\n",
    "    for tweet in array_tweets:\n",
    "        for i in range(0,len(tweet)-1):\n",
    "            jword= tweet[i]+\" \"+tweet[i+1]\n",
    "            if jword in bidict:\n",
    "                bidict.update({jword:(bidict.get(jword)+1)})\n",
    "            else:\n",
    "                bidict.update({jword:1})\n",
    "    return bidict            \n",
    "\n",
    "#calculates the occurence of every single word (unigram) key:unigram value:occurence\n",
    "def unigram_generator(array_tweets):\n",
    "    wdict = {}\n",
    "    for tweet in array_tweets:\n",
    "        for word in tweet:\n",
    "            if word in wdict:\n",
    "                wdict.update({word:(wdict.get(word)+1)})\n",
    "            else:\n",
    "                wdict.update({word:1})\n",
    "    return wdict\n",
    "\n",
    "#generates a tweet from a random bigram\n",
    "def generate_tweet(bigram_dictionary):\n",
    "    starting_point=[key for key in bigram_dictionary if key.startswith(\"<b>\")]\n",
    "    sentence=random.choice(starting_point).split()\n",
    "    while(sentence[-1]!=\"<end>\"):\n",
    "        partial_key=sentence[-1]\n",
    "        max_value=0\n",
    "        max_key=\"<end>\"\n",
    "        for key in bigram_dictionary:\n",
    "            splitted_key=key.split()\n",
    "            if(splitted_key[0] == partial_key):\n",
    "                current_value=bigram_dictionary.get(key)\n",
    "                if(current_value>max_value):\n",
    "                    max_value=current_value\n",
    "                    max_key=splitted_key[1]\n",
    "        sentence.append(max_key)\n",
    "    return sentence\n",
    "\n",
    "#probrability estimation of each trigram with normalization\n",
    "def estimate_probability(bigram_dict,unigram_dict):\n",
    "    for key in bigram_dict:\n",
    "        first_word=key.split()[0]\n",
    "        freq_fw=unigram_dict.get(first_word)\n",
    "        bigram_dict.update({key:((bigram_dict.get(key)+1)/(freq_fw+LEN_VOCABULARY))})\n",
    "    return bigram_dict\n",
    "\n",
    "# add \"Trump posted on\" before the generated tweet\n",
    "def ultimate_tweet(generated_tweet):\n",
    "    str_tweet = ' '.join(map(str,generated_tweet))\n",
    "    current_date = datetime.datetime.now()\n",
    "    current_hour = time.strftime(\"%H:%M\")\n",
    "    str_tweet = str_tweet.replace(\"<b>\", \"Trump posted on \" + str(current_date.day) + \"-\" + str(current_date.month) + \"-\" + str(current_date.year) + \" at \" + str(current_hour) + \" :\")\n",
    "    str_tweet = str_tweet.replace(\"<end>\", \"\")\n",
    "    return str_tweet\n",
    "\n",
    "array_tweets = clean_tweets(tweet_df)\n",
    "array_tweets = add_begin_end(array_tweets) \n",
    "bigram_dict = bigrams_generator(array_tweets)\n",
    "unigram_dict= unigram_generator(array_tweets)\n",
    "\n",
    "bigram_dict = estimate_probability(bigram_dict,unigram_dict)\n",
    "\n",
    "generated_tweet = generate_tweet(bigram_dict)\n",
    "\n",
    "print(ultimate_tweet(generated_tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
